@book{10.5555/984017,
  title = {User Stories Applied: {{For}} Agile Software Development},
  author = {Cohn, Mike},
  year = {2004},
  publisher = {Addison Wesley Longman Publishing Co., Inc.},
  address = {USA},
  abstract = {Agile requirements: discovering what your users really want. With this book, you will learn to: Flexible, quick and practical requirements that work Save time and develop better software that meets users' needs Gathering user stories -- even when you can't talk to users How user stories work, and how they differ from use cases, scenarios, and traditional requirements Leveraging user stories as part of planning, scheduling, estimating, and testing Ideal for Extreme Programming, Scrum, or any other agile methodology ----------------------------------------------------------------------------------------------------------Thoroughly reviewed and eagerly anticipated by the agile community, User Stories Applied offers a requirements process that saves time, eliminates rework, and leads directly to better software.The best way to build software that meets users' needs is to begin with "user stories": simple, clear, brief descriptions of functionality that will be valuable to real users. In User Stories Applied, Mike Cohn provides you with a front-to-back blueprint for writing these user stories and weaving them into your development lifecycle.You'll learn what makes a great user story, and what makes a bad one. You'll discover practical ways to gather user stories, even when you can't speak with your users. Then, once you've compiled your user stories, Cohn shows how to organize them, prioritize them, and use them for planning, management, and testing. User role modeling: understanding what users have in common, and where they differ Gathering stories: user interviewing, questionnaires, observation, and workshops Working with managers, trainers, salespeople and other "proxies" Writing user stories for acceptance testing Using stories to prioritize, set schedules, and estimate release costs Includes end-of-chapter practice questions and exercisesUser Stories Applied will be invaluable to every software developer, tester, analyst, and manager working with any agile method: XP, Scrum... or even your own home-grown approach.ADDISON-WESLEY PROFESSIONALBoston, MA 02116www.awprofessional.comISBN: 0-321-20568-5},
  isbn = {0-321-20568-5},
  file = {/Users/justingebert/Zotero/storage/PLL6BTTY/User-Stories-Applied-Mike-Cohn.pdf}
}

@misc{abrahamssonAgileSoftwareDevelopment2017,
  title = {Agile {{Software Development Methods}}: {{Review}} and {{Analysis}}},
  shorttitle = {Agile {{Software Development Methods}}},
  author = {Abrahamsson, Pekka and Salo, Outi and Ronkainen, Jussi and Warsta, Juhani},
  year = {2017},
  month = sep,
  number = {arXiv:1709.08439},
  eprint = {1709.08439},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.08439},
  urldate = {2025-06-25},
  abstract = {Agile - denoting "the quality of being agile, readiness for motion, nimbleness, activity, dexterity in motion" - software development methods are attempting to offer an answer to the eager business community asking for lighter weight along with faster and nimbler software development processes. This is especially the case with the rapidly growing and volatile Internet software industry as well as for the emerging mobile application environment. The new agile methods have evoked substantial amount of literature and debates. However, academic research on the subject is still scarce, as most of existing publications are written by practitioners or consultants. The aim of this publication is to begin filling this gap by systematically reviewing the existing literature on agile software development methodologies. This publication has three purposes. First, it proposes a definition and a classification of agile software development approaches. Second, it analyses ten software development methods that can be characterized as being "agile" against the defined criterion. Third, it compares these methods and highlights their similarities and differences. Based on this analysis, future research needs are identified and discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/X8ER9IG9/Abrahamsson et al. - 2017 - Agile Software Development Methods Review and Analysis.pdf;/Users/justingebert/Zotero/storage/3FAH9RCQ/1709.html}
}

@article{alamiHowScrumAdds2022,
  title = {How {{Scrum}} Adds Value to Achieving Software Quality?},
  author = {Alami, Adam and Krancher, Oliver},
  year = {2022},
  month = dec,
  journal = {Empirical Software Engineering},
  volume = {27},
  number = {7},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-022-10208-4},
  urldate = {2025-07-20},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/PX4IBS8V/Alami and Krancher - 2022 - How Scrum adds value to achieving software quality.pdf}
}

@misc{anandComprehensiveSurveyAIDriven2024,
  title = {A {{Comprehensive Survey}} of {{AI-Driven Advancements}} and {{Techniques}} in {{Automated Program Repair}} and {{Code Generation}}},
  author = {Anand, Avinash and Gupta, Akshit and Yadav, Nishchay and Bajaj, Shaurya},
  year = {2024},
  month = nov,
  number = {arXiv:2411.07586},
  eprint = {2411.07586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.07586},
  urldate = {2025-06-01},
  abstract = {Bug fixing and code generation have been core research topics in software development for many years. The recent explosive growth in Large Language Models has completely transformed these spaces, putting in reach incredibly powerful tools for both. In this survey, 27 recent papers have been reviewed and split into two groups: one dedicated to Automated Program Repair (APR) and LLM integration and the other to code generation using LLMs. The first group consists of new methods for bug detection and repair, which include locating semantic errors, security vulnerabilities, and runtime failure bugs. The place of LLMs in reducing manual debugging efforts is emphasized in this work by APR toward context-aware fixes, with innovations that boost accuracy and efficiency in automatic debugging. The second group dwells on code generation, providing an overview of both general-purpose LLMs fine-tuned for programming and task-specific models. It also presents methods to improve code generation, such as identifier-aware training, fine-tuning at the instruction level, and incorporating semantic code structures. This survey work contrasts the methodologies in APR and code generation to identify trends such as using LLMs, feedback loops to enable iterative code improvement and open-source models. It also discusses the challenges of achieving functional correctness and security and outlines future directions for research in LLM-based software development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/justingebert/Zotero/storage/PZZKG8R4/Anand et al. - 2024 - A Comprehensive Survey of AI-Driven Advancements and Techniques in Automated Program Repair and Code.pdf;/Users/justingebert/Zotero/storage/HX9EWDJH/2411.html}
}

@misc{AutomatedBugFixing,
  title = {Automated {{Bug Fixing}}: {{From Templates}} to {{AI Agents}}},
  shorttitle = {Automated {{Bug Fixing}}},
  journal = {dzone.com},
  urldate = {2025-03-10},
  abstract = {Automated bug fixing has evolved from simple template-based approaches to sophisticated AI systems powered by LLMs, agents, agentless, and RAG paradigms.},
  howpublished = {https://dzone.com/articles/automated-bug-fixing-from-templates-to-ai-agents},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/6FVM3CC7/automated-bug-fixing-from-templates-to-ai-agents.html}
}

@article{baderGetafixLearningFix2019,
  title = {Getafix: Learning to Fix Bugs Automatically},
  shorttitle = {Getafix},
  author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
  year = {2019},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {OOPSLA},
  pages = {1--27},
  issn = {2475-1421},
  doi = {10.1145/3360585},
  urldate = {2025-03-06},
  abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of an expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect API calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12\% and 91\% of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes. CCS Concepts: {$\bullet$} Software and its engineering {$\rightarrow$} Software testing and debugging.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5LZ3GY8Y/Bader et al. - 2019 - Getafix learning to fix bugs automatically.pdf}
}

@article{bhargavmallampatiRoleGenerativeAI2025,
  title = {The Role of Generative {{AI}} in Software Development: {{Will}} It Replace Developers?},
  shorttitle = {The Role of Generative {{AI}} in Software Development},
  author = {{Bhargav Mallampati}},
  year = {2025},
  month = apr,
  journal = {World Journal of Advanced Research and Reviews},
  volume = {26},
  number = {1},
  pages = {2972--2977},
  issn = {25819615},
  doi = {10.30574/wjarr.2025.26.1.1387},
  urldate = {2025-06-25},
  abstract = {Generative artificial intelligence is fundamentally transforming software development, automating routine tasks while reshaping developer roles and responsibilities within engineering teams. This article explores the impact of generative AI tools like GPT-4, Gemini, and GitHub Copilot on development practices through quantitative analysis across diverse technology companies. The implementation of these AI technologies has demonstrated significant productivity gains in code generation, debugging, refactoring, and testing, with some organizations reporting development cycle reductions exceeding 30\%. However, substantial limitations persist in contextual understanding, security vulnerabilities, architectural decision-making, and code maintainability that necessitate continued human oversight. The integration of AI has catalyzed the emergence of specialized roles focused on prompt engineering, AI validation, and governance frameworks. Rather than replacing developers, generative AI appears to be augmenting human capabilities by handling routine implementation tasks while enabling professionals to focus on higher-value activities requiring creativity, domain knowledge, and critical thinking. Case studies from leading companies reveal successful integration strategies that strategically leverage AI strengths while maintaining human judgment for complex or safety-critical components. This evidence-based assessment provides insights into how AI is reshaping software engineering and the implications for professional developers navigating this transformative paradigm shift.},
  file = {/Users/justingebert/Zotero/storage/ESA6UIKA/Bhargav Mallampati - 2025 - The role of generative AI in software development Will it replace developers.pdf}
}

@misc{bouzeniaRepairAgentAutonomousLLMBased2024,
  title = {{{RepairAgent}}: {{An Autonomous}}, {{LLM-Based Agent}} for {{Program Repair}}},
  shorttitle = {{{RepairAgent}}},
  author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
  year = {2024},
  month = oct,
  number = {arXiv:2403.17134},
  eprint = {2403.17134},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.17134},
  urldate = {2025-06-24},
  abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PRD5M6DG/Bouzenia et al. - 2024 - RepairAgent An Autonomous, LLM-Based Agent for Program Repair.pdf;/Users/justingebert/Zotero/storage/NC4FQHDF/2403.html}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-21},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/Users/justingebert/Zotero/storage/T2LPH5KF/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@misc{changBridgingBugLocalization2025,
  title = {Bridging {{Bug Localization}} and {{Issue Fixing}}: {{A Hierarchical Localization Framework Leveraging Large Language Models}}},
  shorttitle = {Bridging {{Bug Localization}} and {{Issue Fixing}}},
  author = {Chang, Jianming and Zhou, Xin and Wang, Lulu and Lo, David and Li, Bixin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.15292},
  eprint = {2502.15292},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.15292},
  urldate = {2025-03-24},
  abstract = {Automated issue fixing is a critical task in software debugging and has recently garnered significant attention from academia and industry. However, existing fixing techniques predominantly focus on the repair phase, often overlooking the importance of improving the preceding bug localization phase. As a foundational step in issue fixing, bug localization plays a pivotal role in determining the overall effectiveness of the entire process. To enhance the precision of issue fixing by accurately identifying bug locations in large-scale projects, this paper presents BugCerberus, the first hierarchical bug localization framework powered by three customized large language models. First, BugCerberus analyzes intermediate representations of bug-related programs at file, function, and statement levels and extracts bug-related contextual information from the representations. Second, BugCerberus designs three customized LLMs at each level using bug reports and contexts to learn the patterns of bugs. Finally, BugCerberus hierarchically searches for bug-related code elements through well-tuned models to localize bugs at three levels. With BugCerberus, we further investigate the impact of bug localization on the issue fixing. We evaluate BugCerberus on the widely-used benchmark SWE-bench-lite. The experimental results demonstrate that BugCerberus outperforms all baselines. Specifically, at the fine-grained statement level, BugCerberus surpasses the state-of-the-art in Top-N (N=1, 3, 5, 10) by 16.5\%, 5.4\%, 10.2\%, and 23.1\%, respectively. Moreover, in the issue fixing experiments, BugCerberus improves the fix rate of the existing issue fixing approach Agentless by 17.4\% compared to the best baseline, highlighting the significant impact of enhanced bug localization on automated issue fixing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/JYN6JPF5/Chang et al. - 2025 - Bridging Bug Localization and Issue Fixing A Hierarchical Localization Framework Leveraging Large L.pdf;/Users/justingebert/Zotero/storage/QX9UK4WN/2502.html}
}

@article{changSurveyEvaluationLarge2024,
  title = {A {{Survey}} on {{Evaluation}} of {{Large Language Models}}},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  year = {2024},
  month = jun,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {15},
  number = {3},
  pages = {1--45},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3641289},
  urldate = {2025-07-02},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions:               what to evaluate               ,               where to evaluate               , and               how to evaluate               . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:               https://github.com/MLGroupJLU/LLM-eval-survey},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/P2SPBVB3/Chang et al. - 2024 - A Survey on Evaluation of Large Language Models.pdf}
}

@misc{chenUnveilingPitfallsUnderstanding2025,
  title = {Unveiling {{Pitfalls}}: {{Understanding Why AI-driven Code Agents Fail}} at {{GitHub Issue Resolution}}},
  shorttitle = {Unveiling {{Pitfalls}}},
  author = {Chen, Zhi and Ma, Wei and Jiang, Lingxiao},
  year = {2025},
  month = mar,
  number = {arXiv:2503.12374},
  eprint = {2503.12374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.12374},
  urldate = {2025-03-24},
  abstract = {AI-driven software development has rapidly advanced with the emergence of software development agents that leverage large language models (LLMs) to tackle complex, repository-level software engineering tasks. These agents go beyond just generation of final code; they engage in multi-step reasoning, utilize various tools for code modification and debugging, and interact with execution environments to diagnose and iteratively resolve issues. However, most existing evaluations focus primarily on static analyses of final code outputs, yielding limited insights into the agents' dynamic problem-solving processes. To fill this gap, we conduct an in-depth empirical study on 3,977 solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked agents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our exploratory analysis shows that Python execution errors during the issue resolution phase correlate with lower resolution rates and increased reasoning overheads. We have identified the most prevalent errors -- such as ModuleNotFoundError and TypeError -- and highlighted particularly challenging errors like OSError and database-related issues (e.g., IntegrityError) that demand significantly more debugging effort. Furthermore, we have discovered 3 bugs in the SWE-Bench platform that affect benchmark fairness and accuracy; these issues have been reported to and confirmed by the maintainers. To promote transparency and foster future research, we publicly share our datasets and analysis scripts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/WDWRSFMR/Chen et al. - 2025 - Unveiling Pitfalls Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution.pdf;/Users/justingebert/Zotero/storage/WPC2PKQA/2503.html}
}

@book{cohnUserStoriesApplied2013,
  title = {User Stories Applied: For Agile Software Development},
  shorttitle = {User Stories Applied},
  author = {Cohn, Mike},
  year = {2013},
  series = {Addison-{{Wesley}} Signature Series},
  edition = {18. print},
  publisher = {Addison-Wesley},
  address = {Boston, Mass.},
  isbn = {978-0-321-20568-1},
  langid = {english}
}

@misc{ContinuousIntegrationGitHub,
  title = {About Continuous Integration with {{GitHub Actions}}},
  journal = {GitHub Docs},
  urldate = {2025-06-25},
  abstract = {You can create custom continuous integration (CI) workflows directly in your GitHub repository with GitHub Actions.},
  howpublished = {https://docs-internal.github.com/\_next/data/9uQSGns-DWbCy3Cy8blUA/en/free-pro-team\%40latest/actions/concepts/overview/about-continuous-integration-with-github-actions.json?versionId=free-pro-team\%40latest\&productId=actions\&restPage=concepts\&restPage=overview\&restPage=about-continuous-integration-with-github-actions},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/HH53HGIE/about-continuous-integration-with-github-actions.html}
}

@misc{CostPoorSoftware,
  title = {Cost of {{Poor Software Quality}} in the {{U}}.{{S}}.: {{A}} 2022 {{Report}}},
  shorttitle = {Cost of {{Poor Software Quality}} in the {{U}}.{{S}}.},
  journal = {CISQ},
  urldate = {2025-06-24},
  abstract = {The Cost of Poor Software Quality in the US: A 2022 Report seeks to quantify the impact of poor software quality on the United States economy, referencing publicly available source material.},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/WJVDM432/CPSQ-Report-Nov-22-2.pdf;/Users/justingebert/Zotero/storage/F62QT9UX/the-cost-of-poor-quality-software-in-the-us-a-2022-report.html}
}

@misc{DACodeAgentData,
  title = {{{DA-Code}}: {{Agent Data Science Code Generation Benchmark}} for {{Large Language Models}}},
  urldate = {2025-07-03},
  howpublished = {https://da-code-bench.github.io/}
}

@misc{dohmkeGitHubCopilotMeet2025,
  title = {{{GitHub Copilot}}: {{Meet}} the New Coding Agent},
  shorttitle = {{{GitHub Copilot}}},
  author = {Dohmke, Thomas},
  year = {2025},
  month = may,
  journal = {The GitHub Blog},
  urldate = {2025-06-26},
  abstract = {GitHub Copilot has a new feature: a coding agent that can implement a task or issue, run in the background with GitHub Actions, and more.},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/Q4FIBJ8P/github-copilot-meet-the-new-coding-agent.html}
}

@inproceedings{francescoResearchArchitectingMicroservices2017,
  title = {Research on {{Architecting Microservices}}: {{Trends}}, {{Focus}}, and {{Potential}} for {{Industrial Adoption}}},
  shorttitle = {Research on {{Architecting Microservices}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Architecture}} ({{ICSA}})},
  author = {Francesco, Paolo Di and Malavolta, Ivano and Lago, Patricia},
  year = {2017},
  month = apr,
  pages = {21--30},
  doi = {10.1109/ICSA.2017.24},
  urldate = {2025-07-20},
  abstract = {Microservices are a new trend rising fast from the enterprise world. Even though the design principles around microservices have been identified, it is difficult to have a clear view of existing research solutions for architecting microservices. In this paper we apply the systematic mapping study methodology to identify, classify, and evaluate the current state of the art on architecting microservices from the following three perspectives: publication trends, focus of research, and potential for industrial adoption. More specifically, we systematically define a classification framework for categorizing the research on architecting microservices and we rigorously apply it to the 71 selected studies. We synthesize the obtained data and produce a clear overview of the state of the art. This gives a solid basis to plan for future research and applications of architecting microservices.},
  keywords = {Architecture,Computer architecture,Data mining,Market research,Microservices,Service-oriented architecture,Software Architecture,Systematic Mapping Study,Systematics},
  file = {/Users/justingebert/Zotero/storage/JFMXUZWY/Francesco et al. - 2017 - Research on Architecting Microservices Trends, Focus, and Potential for Industrial Adoption.pdf}
}

@article{ghalebEmpiricalStudyLong2019,
  title = {An Empirical Study of the Long Duration of Continuous Integration Builds},
  author = {Ghaleb, Taher Ahmed and Da Costa, Daniel Alencar and Zou, Ying},
  year = {2019},
  month = aug,
  journal = {Empirical Software Engineering},
  volume = {24},
  number = {4},
  pages = {2102--2139},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-019-09695-9},
  urldate = {2025-07-20},
  abstract = {Continuous Integration (CI) is a set of software development practices that allow software development teams to generate software builds more quickly and periodically (e.g., daily or even hourly). CI brings many advantages, such as the early identification of errors when integrating code. When builds are generated frequently, a long build duration may hold developers from performing other important tasks. Recent research has shown that a considerable amount of development time is invested on optimizing the generation of builds. However, the reasons behind long build durations are still vague and need an in-depth study. Our initial investigation shows that many projects have build durations that far exceed the acceptable build duration (i.e., 10 minutes) as reported by recent studies. In this paper, we study several characteristics of CI builds that may be associated with the long duration of CI builds. We perform an empirical study on 104, 442 CI builds from 67 GitHub projects. We use mixed-effects logistic models to model long build durations across projects. Our results reveal that, in addition to common wisdom factors (e.g., project size, team size, build configuration size, and test density), there are other highly important factors to explain long build durations. We observe that rerunning failed commands multiple times is most likely to be associated with long build durations. We also find that builds may run faster if they are configured (a) to cache content that does not change often or (b) to finish as soon as all the required jobs finish. However, we observe that about 40\% of the studied projects do not use or misuse such configurations in their builds.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/55WBFBUI/Ghaleb et al. - 2019 - An empirical study of the long duration of continuous integration builds.pdf}
}

@misc{GitHubActions2025,
  title = {{{GitHub Actions}}},
  year = {2025},
  journal = {GitHub},
  urldate = {2025-07-20},
  abstract = {Easily build, package, release, update, and deploy your project in any language---on GitHub or any external system---without having to run code yourself.},
  howpublished = {https://github.com/features/actions},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/XK6KIPEE/actions.html}
}

@misc{GitHubFeatures2025,
  title = {{{GitHub Features}}},
  year = {2025},
  journal = {GitHub},
  urldate = {2025-07-20},
  abstract = {Get the right tools for the job. Automate your CI/CD and DevOps workflow with GitHub Actions, build securely, manage teams and projects, and review code in one place.},
  howpublished = {https://github.com/features},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/PWNP7S4X/features.html}
}

@inproceedings{gyimesiBugsJSBenchmarkJavaScript2019,
  title = {{{BugsJS}}: A {{Benchmark}} of {{JavaScript Bugs}}},
  shorttitle = {{{BugsJS}}},
  booktitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Gyimesi, P{\'e}ter and Vancsics, B{\'e}la and Stocco, Andrea and Mazinanian, Davood and Besz{\'e}des, {\'A}rp{\'a}d and Ferenc, Rudolf and Mesbah, Ali},
  year = {2019},
  month = apr,
  pages = {90--101},
  issn = {2159-4848},
  doi = {10.1109/ICST.2019.00019},
  urldate = {2025-03-27},
  abstract = {JavaScript is a popular programming language that is also error-prone due to its asynchronous, dynamic, and loosely-typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server-side programs, comprising 444k LOC in total. Each bug is accompanied by its bug report, the test cases that detect it, as well as the patch that fixes it. BugsJS features a rich interface for accessing the faulty and fixed versions of the programs and executing the corresponding test cases, which facilitates conducting highly-reproducible empirical studies and comparisons of JavaScript analysis and testing tools.},
  keywords = {benchmark,Benchmark testing,bug database,BugsJS,Computer bugs,Concurrent computing,JavaScript,literature survey,Maintenance engineering,real bugs,reproducibility,Software,Test pattern generators},
  file = {/Users/justingebert/Zotero/storage/6MVXLEHH/Gyimesi et al. - 2019 - BugsJS a Benchmark of JavaScript Bugs.pdf;/Users/justingebert/Zotero/storage/KPCI53W2/8730197.html}
}

@inproceedings{hiltonUsageCostsBenefits2016,
  title = {Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
  year = {2016},
  month = aug,
  pages = {426--437},
  publisher = {ACM},
  address = {Singapore Singapore},
  doi = {10.1145/2970276.2970358},
  urldate = {2025-07-20},
  abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 opensource projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.},
  copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/6KEUXVDP/Hilton et al. - 2016 - Usage, costs, and benefits of continuous integration in open-source projects.pdf}
}

@article{hossainDeepDiveLarge2024,
  title = {A {{Deep Dive}} into {{Large Language Models}} for {{Automated Bug Localization}} and {{Repair}}},
  author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
  year = {2024},
  month = jul,
  journal = {Proceedings of the ACM on Software Engineering},
  volume = {1},
  number = {FSE},
  pages = {1471--1493},
  issn = {2994-970X},
  doi = {10.1145/3660773},
  urldate = {2025-03-13},
  abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks,                                                               including automated program repair (APR). In this study, we take a deep dive into automated bug localization                                                               and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug                                                               locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach                                                               uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug                                                               fixing. This methodological separation of bug localization and fixing using different LLMs enables effective                                                               integration of diverse contextual information and improved incorporation of inductive biases. We introduce                                                               Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework                                                               that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a                                                               bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We                                                               investigate various styles of prompting to the bug fixing model to identify the most effective prompts that                                                               better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art                                                               (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable                                                               performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our                                                               approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50,                                                               and Top-100 metrics. Besides examining Toggle's generalizability to unseen data, evaluating the effectiveness                                                               of various prompts, we also investigate the impact of additional contextual information such as buggy lines                                                               and code comments on bug localization, and explore the importance of the adjustment model. Our extensive                                                               experiments offer valuable insights and answers to critical research questions.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/3T4HUBN7/Hossain et al. - 2024 - A Deep Dive into Large Language Models for Automated Bug Localization and Repair.pdf}
}

@misc{houLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Software Engineering}}: {{A Systematic Literature Review}}},
  shorttitle = {Large {{Language Models}} for {{Software Engineering}}},
  author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  year = {2024},
  month = apr,
  number = {arXiv:2308.10620},
  eprint = {2308.10620},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.10620},
  urldate = {2025-03-06},
  abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE\_SLR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/9NMUPS2R/Hou et al. - 2024 - Large Language Models for Software Engineering A Systematic Literature Review.pdf;/Users/justingebert/Zotero/storage/48ARVL3M/2308.html}
}

@misc{huCanGPTO1Kill2024,
  title = {Can {{GPT-O1 Kill All Bugs}}? {{An Evaluation}} of {{GPT-Family LLMs}} on {{QuixBugs}}},
  shorttitle = {Can {{GPT-O1 Kill All Bugs}}?},
  author = {Hu, Haichuan and Shang, Ye and Xu, Guolin and He, Congqing and Zhang, Quanjun},
  year = {2024},
  month = dec,
  number = {arXiv:2409.10033},
  eprint = {2409.10033},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.10033},
  urldate = {2025-04-15},
  abstract = {LLMs have long demonstrated remarkable effectiveness in automatic program repair (APR), with OpenAI's ChatGPT being one of the most widely used models in this domain. Through continuous iterations and upgrades of GPT-family models, their performance in fixing bugs has already reached state-of-the-art levels. However, there are few works comparing the effectiveness and variations of different versions of GPT-family models on APR. In this work, inspired by the recent public release of the GPT-o1 models, we conduct the first study to compare the effectiveness of different versions of the GPT-family models in APR. We evaluate the performance of the latest version of the GPT-family models (i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT on APR. We conduct an empirical study of the four GPT-family models against other LLMs and APR techniques on the QuixBugs benchmark from multiple evaluation perspectives, including repair success rate, repair cost, response length, and behavior patterns. The results demonstrate that O1's repair capability exceeds that of prior GPT-family models, successfully fixing all 40 bugs in the benchmark. Our work can serve as a foundation for further in-depth exploration of the applications of GPT-family models in APR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/BZQGPMP2/Hu et al. - 2024 - Can GPT-O1 Kill All Bugs An Evaluation of GPT-Family LLMs on QuixBugs.pdf;/Users/justingebert/Zotero/storage/VDXHEKBG/2409.html}
}

@inproceedings{huoSoftwareQualityAgile2004,
  title = {Software Quality and Agile Methods},
  booktitle = {Proceedings of the 28th {{Annual International Computer Software}} and {{Applications Conference}}, 2004. {{COMPSAC}} 2004.},
  author = {Huo, Ming and Verner, J. and Zhu, Liming and Babar, M.A.},
  year = {2004},
  month = sep,
  pages = {520-525 vol.1},
  issn = {0730-3157},
  doi = {10.1109/CMPSAC.2004.1342889},
  urldate = {2025-07-20},
  abstract = {Agile methods may produce software faster but we also need to know how they meet our quality requirements. In this paper we compare the waterfall model with agile processes to show how agile methods achieve software quality under time pressure and in an unstable requirements environment, i.e. we analyze agile software quality assurance. We present a detailed waterfall model showing its software quality support processes. We then show the quality practices that agile methods have integrated into their processes. This allows us to answer the question "can agile methods ensure quality even though they develop software faster and can handle unstable requirements?".},
  keywords = {Application software,Australia,Collaboration,Computer applications,Continuous improvement,Programming,Quality assurance,Software engineering,Software quality},
  file = {/Users/justingebert/Zotero/storage/MTWKDNEQ/Huo et al. - 2004 - Software quality and agile methods.pdf}
}

@misc{IntroducingCodex,
  title = {Introducing {{Codex}}},
  urldate = {2025-06-26},
  abstract = {Introducing Codex: a cloud-based software engineering agent that can work on many tasks in parallel, powered by codex-1. With Codex, developers can simultaneously deploy multiple agents to independently handle coding tasks such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.},
  howpublished = {https://openai.com/index/introducing-codex/},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/TDD8XJDH/introducing-codex.html}
}

@misc{IntroductionLargeLanguage,
  title = {Introduction to {{Large Language Models}} {\textbar} {{Machine Learning}}},
  journal = {Google for Developers},
  urldate = {2025-07-03},
  howpublished = {https://developers.google.com/machine-learning/resources/intro-llms},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/P9AK5ILB/intro-llms.html}
}

@misc{Issues,
  title = {About Issues},
  journal = {GitHub Docs},
  urldate = {2025-07-20},
  abstract = {Learn how you can use GitHub Issues to track ideas, feedback, tasks, or bugs.},
  howpublished = {https://docs-internal.github.com/\_next/data/LM3KAkw0Ii4E7Nz5N0zHV/en/free-pro-team\%40latest/issues/tracking-your-work-with-issues/about-issues.json?versionId=free-pro-team\%40latest\&productId=issues\&restPage=tracking-your-work-with-issues\&restPage=about-issues},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/YFGSBW7T/about-issues.html}
}

@misc{jimenezSWEbenchCanLanguage2024,
  title = {{{SWE-bench}}: {{Can Language Models Resolve Real-World GitHub Issues}}?},
  shorttitle = {{{SWE-bench}}},
  author = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  year = {2024},
  month = nov,
  number = {arXiv:2310.06770},
  eprint = {2310.06770},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06770},
  urldate = {2025-03-06},
  abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of \$2,294\$ software engineering problems drawn from real GitHub issues and corresponding pull requests across \$12\$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere \$1.96\$\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/CS85ZE5R/Jimenez et al. - 2024 - SWE-bench Can Language Models Resolve Real-World GitHub Issues.pdf;/Users/justingebert/Zotero/storage/LTFAECDB/2310.html}
}

@misc{jinInferFixEndtoEndProgram2023,
  title = {{{InferFix}}: {{End-to-End Program Repair}} with {{LLMs}}},
  shorttitle = {{{InferFix}}},
  author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07263},
  eprint = {2303.07263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07263},
  urldate = {2025-06-24},
  abstract = {Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C\# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6\% for generating fixes in C\# and 76.8\% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/GWPELW9S/Jin et al. - 2023 - InferFix End-to-End Program Repair with LLMs.pdf;/Users/justingebert/Zotero/storage/SKKBGPQ7/2303.html}
}

@misc{jinLLMsLLMbasedAgents2025,
  title = {From {{LLMs}} to {{LLM-based Agents}} for {{Software Engineering}}: {{A Survey}} of {{Current}}, {{Challenges}} and {{Future}}},
  shorttitle = {From {{LLMs}} to {{LLM-based Agents}} for {{Software Engineering}}},
  author = {Jin, Haolin and Huang, Linghan and Cai, Haipeng and Yan, Jun and Li, Bo and Chen, Huaming},
  year = {2025},
  month = apr,
  number = {arXiv:2408.02479},
  eprint = {2408.02479},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.02479},
  urldate = {2025-06-24},
  abstract = {With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PXZAFUCY/Jin et al. - 2025 - From LLMs to LLM-based Agents for Software Engineering A Survey of Current, Challenges and Future.pdf;/Users/justingebert/Zotero/storage/Z4ZLJVPG/2408.html}
}

@inproceedings{justDefects4JDatabaseExisting2014,
  title = {{{Defects4J}}: A Database of Existing Faults to Enable Controlled Testing Studies for {{Java}} Programs},
  shorttitle = {{{Defects4J}}},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Just, Ren{\'e} and Jalali, Darioush and Ernst, Michael D.},
  year = {2014},
  month = jul,
  pages = {437--440},
  publisher = {ACM},
  address = {San Jose CA USA},
  doi = {10.1145/2610384.2628055},
  urldate = {2025-07-04},
  isbn = {978-1-4503-2645-2},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/KBQJEI2U/Just et al. - 2014 - Defects4J a database of existing faults to enable controlled testing studies for Java programs.pdf}
}

@misc{kalliamvakouResearchQuantifyingGitHub2022,
  title = {Research: Quantifying {{GitHub Copilot}}'s Impact on Developer Productivity and Happiness},
  shorttitle = {Research},
  author = {Kalliamvakou, Eirini},
  year = {2022},
  month = sep,
  journal = {The GitHub Blog},
  urldate = {2025-06-26},
  abstract = {When the GitHub Copilot Technical Preview launched just over one year ago, we wanted to know one thing: Is this tool helping developers? The GitHub Next team conducted research using a combination of surveys and experiments, which led us to expected and unexpected answers.},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/JZX85P35/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness.html}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2025-07-21},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sampleefficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/justingebert/Zotero/storage/H9ZY5ERT/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf}
}

@article{kodamasimhamkrishnaExploringSynergyGenerative2024,
  title = {Exploring the Synergy between Generative {{AI}} and Software Engineering: {{Automating}} Code Optimization and Bug Fixing},
  shorttitle = {Exploring the Synergy between Generative {{AI}} and Software Engineering},
  author = {{Kodamasimham Krishna} and {Pranav Murthy} and {Saumya Sarangi}},
  year = {2024},
  month = oct,
  journal = {World Journal of Advanced Engineering Technology and Sciences},
  volume = {13},
  number = {1},
  pages = {682--691},
  issn = {25828266},
  doi = {10.30574/wjaets.2024.13.1.0464},
  urldate = {2025-05-18},
  abstract = {As applied to software engineering, generative AI is quickly transitioning from a zero-sum industry game changer into the primary automation tool for code optimization, bug identification, and problem-solving. This technology takes advantage of artificial intelligence algorithms within machine learning models to analyze and write code, resulting in improved quality and speed of an application development process. The generative AI replenishes productivity in development work and enhances centralization between development work teams through code handling and intelligent suggestions for essential codes. However, the integration of AI in software engineering poses the following problems and ethical questions: the question of accuracy, bias, and data. This paper will review the existing knowledge on generative AI in software engineering regarding its current use, future evolutions and advancements, issues and limitations, and ethical factors in using this technology. This paper considers these aspects to give a global outlook on how generative AI will transform software development in the future and how responsible AI should be employed.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/X7N32X5P/Kodamasimham Krishna et al. - 2024 - Exploring the synergy between generative AI and software engineering Automating code optimization a.pdf}
}

@misc{leeUnifiedDebuggingApproach2024,
  title = {A {{Unified Debugging Approach}} via {{LLM-Based Multi-Agent Synergy}}},
  author = {Lee, Cheryl and Xia, Chunqiu Steven and Yang, Longji and Huang, Jen-tse and Zhu, Zhouruixin and Zhang, Lingming and Lyu, Michael R.},
  year = {2024},
  month = oct,
  number = {arXiv:2404.17153},
  eprint = {2404.17153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.17153},
  urldate = {2025-03-06},
  abstract = {Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, FixAgent, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that FixAgent significantly outperforms state-of-the-art repair methods, fixing 1.25\${\textbackslash}times\$ to 2.56\${\textbackslash}times\$ bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on https://github.com/AcceptePapier/UniDebugger.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/QMBPI5RQ/Lee et al. - 2024 - A Unified Debugging Approach via LLM-Based Multi-Agent Synergy.pdf;/Users/justingebert/Zotero/storage/DEHKRQZS/2404.html}
}

@article{legouesGenProgGenericMethod2012,
  title = {{{GenProg}}: {{A Generic Method}} for {{Automatic Software Repair}}},
  shorttitle = {{{GenProg}}},
  author = {Le Goues, Claire and Nguyen, ThanhVu and Forrest, Stephanie and Weimer, Westley},
  year = {2012},
  month = jan,
  journal = {IEEE Transactions on Software Engineering},
  volume = {38},
  number = {1},
  pages = {54--72},
  issn = {1939-3520},
  doi = {10.1109/TSE.2011.104},
  urldate = {2025-07-03},
  abstract = {This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.},
  keywords = {Automatic programming,Computer bugs,corrections,Debugging,Encoding,Maintenance engineering,Syntactics,testing and debugging.},
  file = {/Users/justingebert/Zotero/storage/SENDSGS6/Le Goues et al. - 2012 - GenProg A Generic Method for Automatic Software Repair.pdf}
}

@article{legouesManyBugsIntroClassBenchmarks2015,
  title = {The {{ManyBugs}} and {{IntroClass Benchmarks}} for {{Automated Repair}} of {{C Programs}}},
  author = {Le Goues, Claire and Holtschulte, Neal and Smith, Edward K. and Brun, Yuriy and Devanbu, Premkumar and Forrest, Stephanie and Weimer, Westley},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {12},
  pages = {1236--1256},
  issn = {1939-3520},
  doi = {10.1109/TSE.2015.2454513},
  urldate = {2025-07-04},
  abstract = {The field of automated software repair lacks a set of common benchmark problems. Although benchmark sets are used widely throughout computer science, existing benchmarks are not easily adapted to the problem of automatic defect repair, which has several special requirements. Most important of these is the need for benchmark programs with reproducible, important defects and a deterministic method for assessing if those defects have been repaired. This article details the need for a new set of benchmarks, outlines requirements, and then presents two datasets, ManyBugs and IntroClass, consisting between them of 1,183 defects in 15 C programs. Each dataset is designed to support the comparative evaluation of automatic repair algorithms asking a variety of experimental questions. The datasets have empirically defined guarantees of reproducibility and benchmark quality, and each study object is categorized to facilitate qualitative evaluation and comparisons by category of bug or program. The article presents baseline experimental results on both datasets for three existing repair methods, GenProg, AE, and TrpAutoRepair, to reduce the burden on researchers who adopt these datasets for their own comparative evaluations.},
  keywords = {Automated program repair,benchmark,Benchmark testing,Computer bugs,Electronic mail,IntroClass,Maintenance engineering,ManyBugs,reproducibility,Software systems,subject defect},
  file = {/Users/justingebert/Zotero/storage/IPHV6WAA/Le Goues et al. - 2015 - The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs.pdf}
}

@inproceedings{linQuixBugsMultilingualProgram2017,
  title = {{{QuixBugs}}: A Multi-Lingual Program Repair Benchmark Set Based on the Quixey Challenge},
  shorttitle = {{{QuixBugs}}},
  booktitle = {Proceedings {{Companion}} of the 2017 {{ACM SIGPLAN International Conference}} on {{Systems}}, {{Programming}}, {{Languages}}, and {{Applications}}: {{Software}} for {{Humanity}}},
  author = {Lin, Derrick and Koppel, James and Chen, Angela and {Solar-Lezama}, Armando},
  year = {2017},
  month = oct,
  pages = {55--56},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3135932.3135941},
  urldate = {2025-04-17},
  isbn = {978-1-4503-5514-8},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/DAPQAHKW/Lin et al. - 2017 - QuixBugs a multi-lingual program repair benchmark set based on the quixey challenge.pdf}
}

@misc{liuMarsCodeAgentAInative2024,
  title = {{{MarsCode Agent}}: {{AI-native Automated Bug Fixing}}},
  shorttitle = {{{MarsCode Agent}}},
  author = {Liu, Yizhou and Gao, Pengfei and Wang, Xinchen and Liu, Jie and Shi, Yexuan and Zhang, Zhao and Peng, Chao},
  year = {2024},
  month = sep,
  number = {arXiv:2409.00899},
  eprint = {2409.00899},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.00899},
  urldate = {2025-03-06},
  abstract = {Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PUZ87WBA/Liu et al. - 2024 - MarsCode Agent AI-native Automated Bug Fixing.pdf;/Users/justingebert/Zotero/storage/DJDUQX4T/2409.html}
}

@misc{liuPromptInjectionAttack2024,
  title = {Prompt {{Injection}} Attack against {{LLM-integrated Applications}}},
  author = {Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang},
  year = {2024},
  month = mar,
  number = {arXiv:2306.05499},
  eprint = {2306.05499},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05499},
  urldate = {2025-07-03},
  abstract = {Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/9KCKMS86/Liu et al. - 2024 - Prompt Injection attack against LLM-integrated Applications.pdf;/Users/justingebert/Zotero/storage/PF3G53IB/2306.html}
}

@misc{LLMsWhatsLarge,
  title = {{{LLMs}}: {{What}}'s a Large Language Model? {\textbar} {{Machine Learning}} {\textbar} {{Google}} for {{Developers}}},
  shorttitle = {{{LLMs}}},
  urldate = {2025-07-03},
  abstract = {Learn how large language models (LLMs) work by using a Transformer architecture and self-attention to generate text.},
  howpublished = {https://developers.google.com/machine-learning/crash-course/llm/transformers},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5895JTQK/transformers.html}
}

@inproceedings{lutellierCoCoNuTCombiningContextaware2020,
  title = {{{CoCoNuT}}: Combining Context-Aware Neural Translation Models Using Ensemble for Program Repair},
  shorttitle = {{{CoCoNuT}}},
  booktitle = {Proceedings of the 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
  year = {2020},
  month = jul,
  pages = {101--114},
  publisher = {ACM},
  address = {Virtual Event USA},
  doi = {10.1145/3395363.3397369},
  urldate = {2025-07-04},
  isbn = {978-1-4503-8008-9},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/PDJ64U9Q/Lutellier et al. - 2020 - CoCoNuT combining context-aware neural translation models using ensemble for program repair.pdf}
}

@inproceedings{margineanSapFixAutomatedEndtoEnd2019,
  title = {{{SapFix}}: {{Automated End-to-End Repair}} at {{Scale}}},
  shorttitle = {{{SapFix}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Marginean, Alexandru and Bader, Johannes and Chandra, Satish and Harman, Mark and Jia, Yue and Mao, Ke and Mols, Alexander and Scott, Andrew},
  year = {2019},
  month = may,
  pages = {269--278},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICSE-SEIP.2019.00039},
  urldate = {2025-03-06},
  abstract = {We report our experience with SAPFIX: the first deployment of automated end-to-end fault fixing, from test case design through to deployed repairs in production code1. We have used SAPFIX at Facebook to repair 6 production systems, each consisting of tens of millions of lines of code, and which are collectively used by hundreds of millions of people worldwide.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-1760-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/TQFLNXJT/Marginean et al. - 2019 - SapFix Automated End-to-End Repair at Scale.pdf}
}

@inproceedings{mechtaevAngelixScalableMultiline2016,
  title = {Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis},
  shorttitle = {Angelix},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Mechtaev, Sergey and Yi, Jooyong and Roychoudhury, Abhik},
  year = {2016},
  month = may,
  pages = {691--701},
  publisher = {ACM},
  address = {Austin Texas},
  doi = {10.1145/2884781.2884807},
  urldate = {2025-07-03},
  isbn = {978-1-4503-3900-1},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/AXIP3W9U/Mechtaev et al. - 2016 - Angelix scalable multiline program patch synthesis via symbolic analysis.pdf}
}

@inproceedings{meemExploringExperiencesAutomated2024,
  title = {Exploring {{Experiences}} with {{Automated Program Repair}} in {{Practice}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Meem, Fairuz Nawer and Smith, Justin and Johnson, Brittany},
  year = {2024},
  month = apr,
  pages = {1--11},
  publisher = {ACM},
  address = {Lisbon Portugal},
  doi = {10.1145/3597503.3639182},
  urldate = {2025-06-26},
  isbn = {979-8-4007-0217-4},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/CEFCDJP4/Meem et al. - 2024 - Exploring Experiences with Automated Program Repair in Practice.pdf}
}

@misc{mengEmpiricalStudyLLMbased2024,
  title = {An {{Empirical Study}} on {{LLM-based Agents}} for {{Automated Bug Fixing}}},
  author = {Meng, Xiangxin and Ma, Zexiong and Gao, Pengfei and Peng, Chao},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10213},
  eprint = {2411.10213},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10213},
  urldate = {2025-03-06},
  abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system's overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PPCPHLM8/Meng et al. - 2024 - An Empirical Study on LLM-based Agents for Automated Bug Fixing.pdf;/Users/justingebert/Zotero/storage/B83LA7R4/2411.html}
}

@inproceedings{mohammedAIDrivenContinuousIntegration2024,
  title = {{{AI-Driven Continuous Integration}} and {{Continuous Deployment}} in {{Software Engineering}}},
  booktitle = {2024 2nd {{International Conference}} on {{Disruptive Technologies}} ({{ICDT}})},
  author = {Mohammed, Abdul Sajid and Saddi, Venkata Ramana and Gopal, Santhosh Kumar and Dhanasekaran, S. and Naruka, Mahaveer Singh},
  year = {2024},
  month = mar,
  pages = {531--536},
  doi = {10.1109/ICDT61202.2024.10489475},
  urldate = {2025-04-23},
  abstract = {AI driven Continuous Integration and Continuous Deployment is a new way of managing and continually updating a software project. This process, powered by Artificial Intelligence, automates the entire software delivery and deployment process - from code submission to monitoring and bug fixing. It eliminates manual errors and allows for multiple versions to be tested in parallel, saving time and effort. By increasing agility, it allows organizations to launch new features to production faster than ever. Continuous Integration and Continuous Deployment leverages artificial intelligence in implementation and execution. It automates the process of integration, testing, packaging and deployment. Furthermore, AI is used to detect and fix bugs which can prevent delays and costly production bugs. AI driven Continuous Integration and Continuous Deployment has become an increasingly popular development strategy. It helps reduce the overall cost and accelerate the software's production cycles, making it easier for developers to quickly get their features and services in the hands of the market.},
  keywords = {Adaptive Systems,Artificial intelligence,Artificial Intelligence,Automation,Cloud Computing,Companies,Computer bugs,Machine Learning,Manuals,Production,Software,Systematics},
  file = {/Users/justingebert/Zotero/storage/6XQC62BK/Mohammed et al. - 2024 - AI-Driven Continuous Integration and Continuous Deployment in Software Engineering.pdf}
}

@misc{naveedComprehensiveOverviewLarge2024,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  year = {2024},
  month = oct,
  number = {arXiv:2307.06435},
  eprint = {2307.06435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.06435},
  urldate = {2025-07-02},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/justingebert/Zotero/storage/PRMWV4XU/Naveed et al. - 2024 - A Comprehensive Overview of Large Language Models.pdf;/Users/justingebert/Zotero/storage/77VCT4H8/2307.html}
}

@misc{PullRequests,
  title = {About Pull Requests},
  journal = {GitHub Docs},
  urldate = {2025-07-20},
  abstract = {Learn about pull requests and draft pull requests on GitHub. Pull requests communicate changes to a branch in a repository. Once a pull request is opened, you can review changes with collaborators and add follow-up commits.},
  howpublished = {https://docs-internal.github.com/\_next/data/LM3KAkw0Ii4E7Nz5N0zHV/en/free-pro-team\%40latest/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests.json?versionId=free-pro-team\%40latest\&productId=pull-requests\&restPage=collaborating-with-pull-requests\&restPage=proposing-changes-to-your-work-with-pull-requests\&restPage=about-pull-requests},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/AWTG44JY/about-pull-requests.html}
}

@inproceedings{puvvadiCodingAgentsComprehensive2025,
  title = {Coding {{Agents}}: {{A Comprehensive Survey}} of {{Automated Bug Fixing Systems}} and {{Benchmarks}}},
  shorttitle = {Coding {{Agents}}},
  booktitle = {2025 {{IEEE}} 14th {{International Conference}} on {{Communication Systems}} and {{Network Technologies}} ({{CSNT}})},
  author = {Puvvadi, Meghana and Arava, Sai Kumar and Santoria, Adarsh and Chennupati, Sesha Sai Prasanna and Puvvadi, Harsha Vardhan},
  year = {2025},
  month = mar,
  pages = {680--686},
  issn = {2473-5655},
  doi = {10.1109/CSNT64827.2025.10968728},
  urldate = {2025-04-27},
  abstract = {One of the trickiest problems in software engineering is automating software issue fixes, which calls for a thorough comprehension of contextual relationships, code semantics, and dynamic debugging techniques. The development of automatic program repair (APR) is examined in this survey, which traces a path from early template and constraint-based approaches to more recent developments powered by large language models (LLMs). Three main paradigms are compared here: retrieval-augmented approaches that integrate external knowledge sources, agent-based systems that use multi-agent frameworks, and agentless systems that use simplified repair pipelines. Real-world benchmarks that mimic actual engineering workflows and repository-level difficulties, such as SWE-bench, CODEAGENT-BENCH, and CodeRAG-Bench, are used to assess these cutting-edge technologies. This study demonstrates how agentic, agentless, and retrieval-augmented systems use LLMs to achieve previously unheard-of precision and scalability by following the shift from localized, single-file solutions to solving complicated, multi-file, and repository-wide difficulties. According to our findings, while complex agent architectures have potential, straightforward test-time scaling frequently produces better outcomes, especially when paired with containerized environments that allow for parallel exploration. Additionally, the survey looks at industrial applications, emphasizing effective connections with quality assurance and DevOps procedures. In order to further the development of more resilient and flexible APR frameworks that blend in perfectly with contemporary software engineering practices, we conclude by highlighting important issues in context handling and validation and suggesting future research directions in improved contextual models, human-AI collaboration, and multi-modal debugging systems.},
  keywords = {Agent-Based Models,AI in Software Development,Automated Program Repair,Benchmark testing,Context modeling,Context-Aware Debugging,Debugging,Debugging Benchmarks,Large language models,Large Language Models,Maintenance engineering,Multi-Agent Systems,Scalability,Semantics,Software,Software engineering,Software Engineering Automation,Surveys},
  file = {/Users/justingebert/Zotero/storage/K4FG4JCB/Puvvadi et al. - 2025 - Coding Agents A Comprehensive Survey of Automated Bug Fixing Systems and Benchmarks.pdf}
}

@misc{rondonEvaluatingAgentbasedProgram2025,
  title = {Evaluating {{Agent-based Program Repair}} at {{Google}}},
  author = {Rondon, Pat and Wei, Renyao and Cambronero, Jos{\'e} and Cito, J{\"u}rgen and Sun, Aaron and Sanyam, Siddhant and Tufano, Michele and Chandra, Satish},
  year = {2025},
  month = jan,
  number = {arXiv:2501.07531},
  eprint = {2501.07531},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.07531},
  urldate = {2025-03-24},
  abstract = {Agent-based program repair offers to automatically resolve complex bugs end-to-end by combining the planning, tool use, and code generation abilities of modern LLMs. Recent work has explored the use of agent-based repair approaches on the popular open-source SWE-Bench, a collection of bugs from highly-rated GitHub Python projects. In addition, various agentic approaches such as SWE-Agent have been proposed to solve bugs in this benchmark. This paper explores the viability of using an agentic approach to address bugs in an enterprise context. To investigate this, we curate an evaluation set of 178 bugs drawn from Google's issue tracking system. This dataset spans both human-reported (78) and machine-reported bugs (100). To establish a repair performance baseline on this benchmark, we implement Passerine, an agent similar in spirit to SWE-Agent that can work within Google's development environment. We show that with 20 trajectory samples and Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e., plausible) for 73\% of machine-reported and 25.6\% of human-reported bugs in our evaluation set. After manual examination, we found that 43\% of machine-reported bugs and 17.9\% of human-reported bugs have at least one patch that is semantically equivalent to the ground-truth patch. These results establish a baseline on an industrially relevant benchmark, which as we show, contains bugs drawn from a different distribution -- in terms of language diversity, size, and spread of changes, etc. -- compared to those in the popular SWE-Bench dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/FVL86YKC/Rondon et al. - 2025 - Evaluating Agent-based Program Repair at Google.pdf;/Users/justingebert/Zotero/storage/XHNQAEUA/2501.html}
}

@article{rupareliaSoftwareDevelopmentLifecycle2010,
  title = {Software Development Lifecycle Models},
  author = {Ruparelia, Nayan B.},
  year = {2010},
  month = may,
  journal = {ACM SIGSOFT Software Engineering Notes},
  volume = {35},
  number = {3},
  pages = {8--13},
  issn = {0163-5948},
  doi = {10.1145/1764810.1764814},
  urldate = {2025-06-25},
  abstract = {This history column article provides a tour of the main software development life cycle (SDLC) models. (A lifecycle covers all the stages of software from its inception with requirements definition through to fielding and maintenance.) System development lifecycle models have drawn heavily on software and so the two terms can be used interchangeably in terms of SDLC, especially since software development in this respect encompasses software systems development. Because the merits of selecting and using an SDLC vary according to the environment in which software is developed as well as its application, I discuss three broad categories for consideration when analyzing the relative merits of SDLC models. I consider the waterfall model before the other models because it has had a profound effect on software development, and has additionally influenced many SDLC models prevalent today. Thereafter, I consider some of the mainstream models and finish with a discussion of what the future could hold for SDLC models.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/UNNBC4VT/Ruparelia - 2010 - Software development lifecycle models.pdf}
}

@article{sarscharPipelineAutomatedCode,
  title = {Pipeline for {{Automated Code Generation}} from {{Backlog Items}} ({{PACGBI}}) -- {{Analysis}} of {{Potentials}} and {{Limitations}} of {{Generative AI}} for {{Web Development}}},
  author = {Sarschar, Mahja},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/KIIMQ946/Sarschar - Pipeline for Automated Code Generation from Backlog Items (PACGBI)  Analysis of Potentials and Limi.pdf}
}

@article{sauvolaFutureSoftwareDevelopment2024,
  title = {Future of Software Development with Generative {{AI}}},
  author = {Sauvola, Jaakko and Tarkoma, Sasu and Klemettinen, Mika and Riekki, Jukka and Doermann, David},
  year = {2024},
  month = may,
  journal = {Automated Software Engineering},
  volume = {31},
  number = {1},
  pages = {26},
  issn = {0928-8910, 1573-7535},
  doi = {10.1007/s10515-024-00426-z},
  urldate = {2025-06-25},
  abstract = {Generative AI is regarded as a major disruption to software development. Platforms, repositories, clouds, and the automation of tools and processes have been proven to improve productivity, cost, and quality. Generative AI, with its rapidly expanding capabilities, is a major step forward in this field. As a new key enabling technology, it can be used for many purposes, from creative dimensions to replacing repetitive and manual tasks. The number of opportunities increases with the capabilities of large-language models (LLMs). This has raised concerns about ethics, education, regulation, intellectual property, and even criminal activities. We analyzed the potential of generative AI and LLM technologies for future software development paths. We propose four primary scenarios, model trajectories for transitions between them, and reflect against relevant software development operations. The motivation for this research is clear: the software development industry needs new tools to understand the potential, limitations, and risks of generative AI, as well as guidelines for using it.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/MYE9M86B/Sauvola et al. - 2024 - Future of software development with generative AI.pdf}
}

@inproceedings{sobaniaAnalysisAutomaticBug2023,
  title = {An {{Analysis}} of the {{Automatic Bug Fixing Performance}} of {{ChatGPT}}},
  booktitle = {2023 {{IEEE}}/{{ACM International Workshop}} on {{Automated Program Repair}} ({{APR}})},
  author = {Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
  year = {2023},
  month = may,
  pages = {23--30},
  doi = {10.1109/APR59189.2023.00012},
  urldate = {2025-03-06},
  abstract = {To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the performance with the results of several other approaches reported in the literature. We find that ChatGPT's bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
  keywords = {Automated program repair,automatic bug fixing,Benchmark testing,Chatbots,ChatGPT,Codex,Computer bugs,Deep learning,language models,Maintenance engineering,Navigation,Source coding},
  file = {/Users/justingebert/Zotero/storage/86DUHN6D/Sobania et al. - 2023 - An Analysis of the Automatic Bug Fixing Performance of ChatGPT.pdf;/Users/justingebert/Zotero/storage/EQ7Z8GHT/10189263.html}
}

@misc{staffOctoverseAILeads2024,
  title = {Octoverse: {{AI}} Leads {{Python}} to Top Language as the Number of Global Developers Surges},
  shorttitle = {Octoverse},
  author = {Staff, GitHub},
  year = {2024},
  month = oct,
  journal = {The GitHub Blog},
  urldate = {2025-07-20},
  abstract = {In this year's Octoverse report, we study how public and open source activity on GitHub shows how AI is expanding as the global developer community surges in size.},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/69KHYZTN/octoverse-2024.html}
}

@misc{sunCoInCountingInvisible2025,
  title = {{{CoIn}}: {{Counting}} the {{Invisible Reasoning Tokens}} in {{Commercial Opaque LLM APIs}}},
  shorttitle = {{{CoIn}}},
  author = {Sun, Guoheng and Wang, Ziyao and Tian, Bowei and Liu, Meng and Shen, Zheyu and He, Shwai and He, Yexiao and Ye, Wanghao and Wang, Yiting and Li, Ang},
  year = {2025},
  month = may,
  number = {arXiv:2505.13778},
  eprint = {2505.13778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13778},
  urldate = {2025-07-19},
  abstract = {As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7\%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/justingebert/Zotero/storage/YW4IVW6N/Sun et al. - 2025 - CoIn Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs.pdf;/Users/justingebert/Zotero/storage/MJ3QYFKN/2505.html}
}

@misc{sunCoInCountingInvisible2025a,
  title = {{{CoIn}}: {{Counting}} the {{Invisible Reasoning Tokens}} in {{Commercial Opaque LLM APIs}}},
  shorttitle = {{{CoIn}}},
  author = {Sun, Guoheng and Wang, Ziyao and Tian, Bowei and Liu, Meng and Shen, Zheyu and He, Shwai and He, Yexiao and Ye, Wanghao and Wang, Yiting and Li, Ang},
  year = {2025},
  month = may,
  number = {arXiv:2505.13778},
  eprint = {2505.13778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13778},
  urldate = {2025-07-19},
  abstract = {As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7\%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/justingebert/Zotero/storage/JX28553I/Sun et al. - 2025 - CoIn Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs.pdf;/Users/justingebert/Zotero/storage/83MAGXVN/2505.html}
}

@misc{sunCoInCountingInvisible2025b,
  title = {{{CoIn}}: {{Counting}} the {{Invisible Reasoning Tokens}} in {{Commercial Opaque LLM APIs}}},
  shorttitle = {{{CoIn}}},
  author = {Sun, Guoheng and Wang, Ziyao and Tian, Bowei and Liu, Meng and Shen, Zheyu and He, Shwai and He, Yexiao and Ye, Wanghao and Wang, Yiting and Li, Ang},
  year = {2025},
  month = may,
  number = {arXiv:2505.13778},
  eprint = {2505.13778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13778},
  urldate = {2025-07-19},
  abstract = {As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7\%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/justingebert/Zotero/storage/P2TXDPV8/Sun et al. - 2025 - CoIn Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs.pdf;/Users/justingebert/Zotero/storage/DQRDL5T6/2505.html}
}

@misc{sunCoInCountingInvisible2025d,
  title = {{{CoIn}}: {{Counting}} the {{Invisible Reasoning Tokens}} in {{Commercial Opaque LLM APIs}}},
  shorttitle = {{{CoIn}}},
  author = {Sun, Guoheng and Wang, Ziyao and Tian, Bowei and Liu, Meng and Shen, Zheyu and He, Shwai and He, Yexiao and Ye, Wanghao and Wang, Yiting and Li, Ang},
  year = {2025},
  month = may,
  number = {arXiv:2505.13778},
  eprint = {2505.13778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13778},
  urldate = {2025-07-19},
  abstract = {As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7\%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/justingebert/Zotero/storage/TRKINQAG/Sun et al. - 2025 - CoIn Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs.pdf;/Users/justingebert/Zotero/storage/H45BP25C/2505.html}
}

@article{tangLargeLanguageModels2024,
  title = {Large {{Language Models Meet Automated Program Repair}}: {{Innovations}}, {{Challenges}} and {{Solutions}}},
  shorttitle = {Large {{Language Models Meet Automated Program Repair}}},
  author = {Tang, Yiting},
  year = {2024},
  month = dec,
  journal = {Applied and Computational Engineering},
  volume = {117},
  number = {1},
  pages = {22--30},
  issn = {2755-2721, 2755-273X},
  doi = {10.54254/2755-2721/2024.18303},
  urldate = {2025-06-01},
  abstract = {As the field of Automated Program Repair (APR) continues to evolve, traditional Neural Program Repair (NPR) methods, while successful in low-resource computing scenarios, still confront numerous challenges, including the demand for extensive training data, the limited generality of specially designed networks, and a lack of robustness. In recent years, Large Language Models (LLMs) have demonstrated remarkable efficacy in downstream code-related tasks, thanks to their potent comprehension and text generation capabilities, gradually emerging as pivotal tools in automated program repair. Compared to NPR techniques, LLM-based APRs exhibit superior repair performance and enhanced generality, leading to their increasing adoption in APR tasks. Currently, the performance of zero-shot LLM-based APRs has surpassed that of NPR. LLM-based APRs have issues, such as excessive fine-tuning costs, data leakage concerns, and a shortage of domain-specific knowledge. This paper aims to review and summarize the latest advancements in LLM-based APRs from the perspectives of innovation, challenges, and solutions, providing researchers with profound insights and future directions.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/55V4UMVI/Tang - 2024 - Large Language Models Meet Automated Program Repair Innovations, Challenges and Solutions.pdf}
}

@misc{tihanyiNewEraSoftware2024,
  title = {A {{New Era}} in {{Software Security}}: {{Towards Self-Healing Software}} via {{Large Language Models}} and {{Formal Verification}}},
  shorttitle = {A {{New Era}} in {{Software Security}}},
  author = {Tihanyi, Norbert and Jain, Ridhi and Charalambous, Yiannis and Ferrag, Mohamed Amine and Sun, Youcheng and Cordeiro, Lucas C.},
  year = {2024},
  month = jun,
  number = {arXiv:2305.14752},
  eprint = {2305.14752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14752},
  urldate = {2025-06-26},
  abstract = {This paper introduces an innovative approach that combines Large Language Models (LLMs) with Formal Verification strategies for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. These counterexamples are supported by mathematical proofs and the stack trace of the vulnerabilities. Using a specially designed prompt, we combine the original source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50,000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI's capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/TAFRNV6P/Tihanyi et al. - 2024 - A New Era in Software Security Towards Self-Healing Software via Large Language Models and Formal V.pdf;/Users/justingebert/Zotero/storage/J8WEUF9A/2305.html}
}

@inproceedings{tregubovImpactTaskSwitching2017,
  title = {Impact of Task Switching and Work Interruptions on Software Development Processes},
  booktitle = {Proceedings of the 2017 {{International Conference}} on {{Software}} and {{System Process}}},
  author = {Tregubov, Alexey and Boehm, Barry and Rodchenko, Natalia and Lane, Jo Ann},
  year = {2017},
  month = jul,
  pages = {134--138},
  publisher = {ACM},
  address = {Paris France},
  doi = {10.1145/3084100.3084116},
  urldate = {2025-06-23},
  isbn = {978-1-4503-5270-3},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/K5372KAG/Tregubov et al. - 2017 - Impact of task switching and work interruptions on software development processes.pdf}
}

@article{tufanoEmpiricalStudyLearning2019,
  title = {An {{Empirical Study}} on {{Learning Bug-Fixing Patches}} in the {{Wild}} via {{Neural Machine Translation}}},
  author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
  year = {2019},
  month = oct,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {28},
  number = {4},
  pages = {1--29},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3340544},
  urldate = {2025-04-08},
  abstract = {Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50\% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/DG9PG5SN/Tufano et al. - 2019 - An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation.pdf}
}

@article{ugwuezeContinuousIntegrationDeployment2024,
  title = {Continuous {{Integration}} and {{Deployment Strategies}} for {{Streamlined DevOps}} in {{Software Engineering}} and {{Application Delivery}}},
  author = {Ugwueze, Vincent and Chukwunweike, Joseph},
  year = {2024},
  month = jan,
  journal = {International Journal of Computer Applications Technology and Research},
  pages = {1--24},
  doi = {10.7753/IJCATR1401.1001},
  abstract = {In modern software engineering, Continuous Integration (CI) and Continuous Deployment (CD) have emerged as essential practices for improving the efficiency and reliability of software delivery. These practices form the backbone of DevOps, a set of methodologies that bridges the gap between development and operations, fostering collaboration and automating the delivery pipeline. The concept of CI involves the frequent integration of code changes into a shared repository, allowing for early detection of bugs and ensuring that new code aligns with the project's standards. CD extends this by automating the deployment of code changes into production, enabling frequent and reliable releases without manual intervention. This paper explores the strategies and tools that enable seamless integration and deployment in software engineering. It examines the role of version control systems, automated testing, and containerization technologies such as Docker in optimizing CI/CD workflows. The challenges associated with scaling CI/CD pipelines, handling microservices architectures, and maintaining security throughout the deployment process are discussed in detail. Additionally, this paper highlights the importance of monitoring and feedback loops for continuous improvement and the adoption of best practices in DevOps, such as automation, collaboration, and rapid iteration. By embracing CI/CD strategies, organizations can reduce time-to-market, enhance software quality, and increase deployment frequency, ultimately streamlining DevOps processes and accelerating application delivery. This paper provides insights into the transformative impact of CI/CD practices on the software engineering lifecycle, offering practical approaches for successful implementation.},
  file = {/Users/justingebert/Zotero/storage/S4BEYU4N/Ugwueze and Chukwunweike - 2024 - Continuous Integration and Deployment Strategies for Streamlined DevOps in Software Engineering and.pdf}
}

@misc{UnderstandingGitHubActions,
  title = {Understanding {{GitHub Actions}}},
  journal = {GitHub Docs},
  urldate = {2025-07-20},
  abstract = {Learn the basics of core concepts and essential terminology in GitHub Actions.},
  howpublished = {https://docs-internal.github.com/\_next/data/LM3KAkw0Ii4E7Nz5N0zHV/en/free-pro-team\%40latest/actions/get-started/understanding-github-actions.json?versionId=free-pro-team\%40latest\&productId=actions\&restPage=get-started\&restPage=understanding-github-actions},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/24NSGJPK/understanding-github-actions.html}
}

@misc{UsabilityTestAdhstudy,
  title = {Usability-{{Test}}: Adhstudy},
  urldate = {2025-07-17},
  howpublished = {https://snahn2209.github.io/adhstudy-test/},
  file = {/Users/justingebert/Zotero/storage/S92N8WKN/adhstudy-test.html}
}

@misc{UsingWorkflowRun,
  title = {Using Workflow Run Logs},
  journal = {GitHub Docs},
  urldate = {2025-07-20},
  abstract = {You can view, search, and download the logs for each job in a workflow run.},
  howpublished = {https://docs-internal.github.com/en/actions/how-tos/monitoring-and-troubleshooting-workflows/monitoring-workflows/using-workflow-run-logs},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/VIBL85R2/using-workflow-run-logs.html}
}

@inproceedings{vasilescuSkyNotLimit2016,
  title = {The Sky Is Not the Limit: Multitasking across {{GitHub}} Projects},
  shorttitle = {The Sky Is Not the Limit},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Vasilescu, Bogdan and Blincoe, Kelly and Xuan, Qi and Casalnuovo, Casey and Damian, Daniela and Devanbu, Premkumar and Filkov, Vladimir},
  year = {2016},
  month = may,
  pages = {994--1005},
  publisher = {ACM},
  address = {Austin Texas},
  doi = {10.1145/2884781.2884875},
  urldate = {2025-06-24},
  isbn = {978-1-4503-3900-1},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/Z6VCTLHA/Vasilescu et al. - 2016 - The sky is not the limit multitasking across GitHub projects.pdf}
}

@misc{wangEmpiricalResearchUtilizing2025,
  title = {Empirical {{Research}} on {{Utilizing LLM-based Agents}} for {{Automated Bug Fixing}} via {{LangGraph}}},
  author = {Wang, Jialin and Duan, Zhihua},
  year = {2025},
  month = jan,
  publisher = {Computer Science},
  doi = {10.33774/coe-2025-jbpg6},
  urldate = {2025-03-12},
  abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components---LangGraph, GLM-4-Flash, and ChromaDB---within a four-step iterative workflow to deliver robust performance and seamless functionality.},
  archiveprefix = {Computer Science},
  copyright = {https://www.cambridge.org/engage/coe/legal-information?show=terms-of-use},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/PAEXZG5F/Wang and Duan - 2025 - Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph.pdf}
}

@misc{wangSoftwareDevelopmentLife2025,
  title = {Software {{Development Life Cycle Perspective}}: {{A Survey}} of {{Benchmarks}} for {{Code Large Language Models}} and {{Agents}}},
  shorttitle = {Software {{Development Life Cycle Perspective}}},
  author = {Wang, Kaixin and Li, Tianlin and Zhang, Xiaoyu and Wang, Chong and Sun, Weisong and Liu, Yang and Shi, Bin},
  year = {2025},
  month = may,
  number = {arXiv:2505.05283},
  eprint = {2505.05283},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.05283},
  urldate = {2025-07-04},
  abstract = {Code large language models (CodeLLMs) and agents have shown great promise in tackling complex software engineering tasks.Compared to traditional software engineering methods, CodeLLMs and agents offer stronger abilities, and can flexibly process inputs and outputs in both natural and code. Benchmarking plays a crucial role in evaluating the capabilities of CodeLLMs and agents, guiding their development and deployment. However, despite their growing significance, there remains a lack of comprehensive reviews of benchmarks for CodeLLMs and agents. To bridge this gap, this paper provides a comprehensive review of existing benchmarks for CodeLLMs and agents, studying and analyzing 181 benchmarks from 461 relevant papers, covering the different phases of the software development life cycle (SDLC). Our findings reveal a notable imbalance in the coverage of current benchmarks, with approximately 60\% focused on the software development phase in SDLC, while requirements engineering and software design phases receive minimal attention at only 5\% and 3\%, respectively. Additionally, Python emerges as the dominant programming language across the reviewed benchmarks. Finally, this paper highlights the challenges of current research and proposes future directions, aiming to narrow the gap between the theoretical capabilities of CodeLLMs and agents and their application in real-world scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/3ZGS8RHF/Wang et al. - 2025 - Software Development Life Cycle Perspective A Survey of Benchmarks for Code Large Language Models a.pdf;/Users/justingebert/Zotero/storage/6UXIU5QQ/2505.html}
}

@misc{WhatGenerativeAI2021,
  title = {What Is Generative {{AI}}?},
  year = {2021},
  month = feb,
  journal = {IBM Research},
  urldate = {2025-07-20},
  abstract = {Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.},
  copyright = {{\copyright} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/what-is-generative-AI},
  langid = {american},
  file = {/Users/justingebert/Zotero/storage/H9MNXZLP/what-is-generative-AI.html}
}

@article{winterHowDevelopersReally2023,
  title = {How Do {{Developers Really Feel About Bug Fixing}}? {{Directions}} for {{Automatic Program Repair}}},
  shorttitle = {How Do {{Developers Really Feel About Bug Fixing}}?},
  author = {Winter, Emily and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, S{\ae}mundur and Nowack, Vesna and Woodward, John},
  year = {2023},
  month = apr,
  journal = {IEEE Transactions on Software Engineering},
  volume = {49},
  number = {4},
  pages = {1823--1841},
  issn = {1939-3520},
  doi = {10.1109/TSE.2022.3194188},
  urldate = {2025-06-24},
  abstract = {Automatic program repair (APR) is a rapidly advancing field of software engineering that aims to supplement or replace manual bug fixing with an automated tool. For APR to be successfully adopted in industry, it is vital that APR tools respond to developer needs and preferences. However, very little research has considered developers' general attitudes to APR or developers' current bug fixing practices (the activity APR aims to replace). This article responds to this gap by reporting on a survey of 386 software developers about their bug finding and fixing practices and experiences, and their instinctive attitudes towards APR. We find that bug finding and fixing is not necessarily as onerous for developers as has often been suggested, being rated as more satisfying than developers' general work. The fact that developers derive satisfaction and benefit from bug fixing indicates that APR adoption is not as simple as APR replacing an unwanted activity. When it comes to potential APR approaches, we find a strong preference for developers being kept in the loop (for example, choosing between different fixes or validating fixes) as opposed to a fully automated process. This suggests that advances in APR should be careful to consider the agency of the developer, as well as what information is presented to developers alongside fixes. It also indicates that there are key barriers related to trust that would need to be overcome for full scale APR adoption, supported by the fact that even those developers who stated that they were positive about APR listed several caveats and concerns. We find very few statistically significant relationships between particular demographic variables (for example, developer experience, age, education) and key attitudinal variables, suggesting that developers' instinctive attitudes towards APR are little influenced by experience level but are held widely across the developer community.},
  keywords = {Automation,Computer bugs,Debugging,Maintenance engineering,Manuals,Software,Task analysis},
  file = {/Users/justingebert/Zotero/storage/7UKXAVH5/Winter et al. - 2023 - How do Developers Really Feel About Bug Fixing Directions for Automatic Program Repair.pdf}
}

@misc{Workflows,
  title = {About Workflows},
  journal = {GitHub Docs},
  urldate = {2025-06-25},
  abstract = {Get a high-level overview of GitHub Actions workflows, including triggers, syntax, and advanced features.},
  howpublished = {https://docs-internal.github.com/\_next/data/9uQSGns-DWbCy3Cy8blUA/en/free-pro-team\%40latest/actions/concepts/workflows-and-actions/about-workflows.json?versionId=free-pro-team\%40latest\&productId=actions\&restPage=concepts\&restPage=workflows-and-actions\&restPage=about-workflows},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/7MXXARY6/about-workflows.html}
}

@misc{xiaAgentlessDemystifyingLLMbased2024,
  title = {Agentless: {{Demystifying LLM-based Software Engineering Agents}}},
  shorttitle = {Agentless},
  author = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  year = {2024},
  month = oct,
  number = {arXiv:2407.01489},
  eprint = {2407.01489},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01489},
  urldate = {2025-04-24},
  abstract = {Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00\%, 96 correct fixes) and low cost (\$0.70) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/2IPAZLQS/Xia et al. - 2024 - Agentless Demystifying LLM-based Software Engineering Agents.pdf;/Users/justingebert/Zotero/storage/9CAYV9Y4/2407.html}
}

@inproceedings{xiaAutomatedProgramRepair2023,
  title = {Automated {{Program Repair}} in the {{Era}} of {{Large Pre-trained Language Models}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  year = {2023},
  month = may,
  pages = {1482--1494},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/ICSE48619.2023.00129},
  urldate = {2025-06-19},
  abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bugfixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-5701-9},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/3LFIYMLQ/Xia et al. - 2023 - Automated Program Repair in the Era of Large Pre-trained Language Models.pdf}
}

@inproceedings{xiaAutomatedProgramRepair2024,
  title = {Automated {{Program Repair}} via {{Conversation}}: {{Fixing}} 162 out of 337 {{Bugs}} for \$0.42 {{Each}} Using {{ChatGPT}}},
  shorttitle = {Automated {{Program Repair}} via {{Conversation}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Xia, Chunqiu Steven and Zhang, Lingming},
  year = {2024},
  month = sep,
  pages = {819--831},
  publisher = {ACM},
  address = {Vienna Austria},
  doi = {10.1145/3650212.3680323},
  urldate = {2025-05-12},
  isbn = {979-8-4007-0612-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5VD7NFLZ/Xia and Zhang - 2024 - Automated Program Repair via Conversation Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT.pdf}
}

@inproceedings{xiaLessTrainingMore2022,
  title = {Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning},
  shorttitle = {Less Training, More Repairing Please},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Xia, Chunqiu Steven and Zhang, Lingming},
  year = {2022},
  month = nov,
  pages = {959--971},
  publisher = {ACM},
  address = {Singapore Singapore},
  doi = {10.1145/3540250.3549101},
  urldate = {2025-07-04},
  isbn = {978-1-4503-9413-0},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/7DBGKYYY/Xia and Zhang - 2022 - Less training, more repairing please revisiting automated program repair via zero-shot learning.pdf}
}

@misc{yangSWEagentAgentComputerInterfaces2024,
  title = {{{SWE-agent}}: {{Agent-Computer Interfaces Enable Automated Software Engineering}}},
  shorttitle = {{{SWE-agent}}},
  author = {Yang, John and Jimenez, Carlos E. and Wettig, Alexander and Lieret, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
  year = {2024},
  month = nov,
  number = {arXiv:2405.15793},
  eprint = {2405.15793},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15793},
  urldate = {2025-04-20},
  abstract = {Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5\% and 87.7\%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/CE2TI6N9/Yang et al. - 2024 - SWE-agent Agent-Computer Interfaces Enable Automated Software Engineering.pdf;/Users/justingebert/Zotero/storage/V37ZJVGM/2405.html}
}

@article{yaoREACSYNERGIZINGREASONING2023,
  title = {{{REAC T}}: {{SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/YGV27J2I/Yao et al. - 2023 - REAC T SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS.pdf}
}

@inproceedings{yinThinkRepairSelfDirectedAutomated2024,
  title = {{{ThinkRepair}}: {{Self-Directed Automated Program Repair}}},
  shorttitle = {{{ThinkRepair}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
  year = {2024},
  month = sep,
  pages = {1274--1286},
  publisher = {ACM},
  address = {Vienna Austria},
  doi = {10.1145/3650212.3680359},
  urldate = {2025-03-12},
  isbn = {979-8-4007-0612-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5BBJV22R/Yin et al. - 2024 - ThinkRepair Self-Directed Automated Program Repair.pdf}
}

@article{zayatFrameworkStudyAgile2020,
  title = {Framework {{Study}} for {{Agile Software Development Via Scrum}} and {{Kanban}}},
  author = {Zayat, Wael and Senvar, Ozlem},
  year = {2020},
  month = jun,
  journal = {International Journal of Innovation and Technology Management},
  volume = {17},
  number = {04},
  publisher = {World Scientific Pub Co Pte Ltd},
  issn = {0219-8770, 1793-6950},
  doi = {10.1142/s0219877020300025},
  urldate = {2025-07-20},
  abstract = {This paper provides a systematic comparison between two well-known Agile methodologies: Scrum, which is a framework of doing projects by allocating tasks into small stages called sprints, and Kanban, which is a scheduling system to manage the flow of work by means of visual signals. In this regard, both methodologies were reviewed to explore similarities and differences between them. Then, a focus group survey was performed to specify the preferable methodology for product development according to various parameters in the project environment including project complexity, level of uncertainty, and work size with consideration of output factors like quality, productivity, and delivery. Results show the flexibility of both methodologies in approaching Agile objectives, where Scrum emphasizes on the corporation of the customer and development teams with a focus on particular skills such as planning, organization, presentation, and reviewing which makes it ideal for new and complex projects where a regular involvement of the customer is required, whereas Kanban is more operative in continuous-flow environments with a steady approach toward a system improvement.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/BW4CXL38/Zayat and Senvar - 2020 - Framework Study for Agile Software Development Via Scrum and Kanban.pdf}
}

@inproceedings{zhangEmpiricalStudyFactors2012,
  title = {An {{Empirical Study}} on {{Factors Impacting Bug Fixing Time}}},
  booktitle = {2012 19th {{Working Conference}} on {{Reverse Engineering}}},
  author = {Zhang, Feng and Khomh, Foutse and Zou, Ying and Hassan, Ahmed E.},
  year = {2012},
  month = oct,
  pages = {225--234},
  issn = {2375-5369},
  doi = {10.1109/WCRE.2012.32},
  urldate = {2025-06-24},
  abstract = {Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process.},
  keywords = {bug fixing process,bug report,change request,Computer bugs,Delay,Distributed Bragg reflectors,empirical software engineering,fixing time,History,Logistics,mylyn,Operating systems},
  file = {/Users/justingebert/Zotero/storage/ZP8HTXH8/Zhang et al. - 2012 - An Empirical Study on Factors Impacting Bug Fixing Time.pdf}
}

@article{zhangPATCHEmpoweringLarge2025,
  title = {{{PATCH}}: {{Empowering Large Language Model}} with {{Programmer-Intent Guidance}} and {{Collaborative-Behavior Simulation}} for {{Automatic Bug Fixing}}},
  shorttitle = {{{PATCH}}},
  author = {Zhang, Yuwei and Jin, Zhi and Xing, Ying and Li, Ge and Liu, Fang and Zhu, Jiaxin and Dou, Wensheng and Wei, Jun},
  year = {2025},
  month = feb,
  journal = {ACM Transactions on Software Engineering and Methodology},
  pages = {3718739},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3718739},
  urldate = {2025-03-24},
  abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/VFST3JBC/Zhang et al. - 2025 - PATCH Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Si.pdf}
}

@article{zhangSTEAMSimulatingInTeractive2025,
  title = {{{STEAM}}: {{Simulating}} the {{InTeractive BEhavior}} of {{ProgrAMmers}} for {{Automatic Bug Fixing}}},
  shorttitle = {{{STEAM}}},
  author = {Zhang, Yuwei and Jin, Zhi and Xing, Ying and Li, Ge},
  year = {2025},
  month = feb,
  journal = {ACM Transactions on Software Engineering and Methodology},
  eprint = {2308.14460},
  primaryclass = {cs},
  pages = {3718739},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3718739},
  urldate = {2025-03-06},
  abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made notable progress in exploring the potential of large language models (LLMs) for automatic bug fixing. However, existing studies often overlook the collaborative nature of bug resolution, treating it as a single-stage process. To overcome this limitation, we introduce a novel stage-wise framework named STEAM in this paper. The objective of STEAM is to simulate the interactive behavior of multiple programmers involved in various stages across the bug's life cycle. Taking inspiration from bug management practices, we decompose the bug fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to imitate the collaborative abilities of programmers during the resolution of software bugs. By harnessing the collective contribution, STEAM effectively enhances the bug-fixing capabilities of LLMs. We implement STEAM by employing the powerful dialogue-based LLM -- ChatGPT. Our evaluation on the widely adopted bug-fixing benchmark demonstrates that STEAM has achieved a new state-of-the-art level of bug-fixing performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/VLVR2KIN/Zhang et al. - 2025 - STEAM Simulating the InTeractive BEhavior of ProgrAMmers for Automatic Bug Fixing.pdf;/Users/justingebert/Zotero/storage/YPJKESGG/2308.html}
}

@article{zhangSurveyLearningbasedAutomated2024,
  title = {A {{Survey}} of {{Learning-based Automated Program Repair}}},
  author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
  year = {2024},
  month = feb,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {33},
  number = {2},
  pages = {1--69},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3631974},
  urldate = {2025-06-26},
  abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.                            In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository:               https://github.com/iSEngLab/AwesomeLearningAPR               .},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/SJRTIEM9/Zhang et al. - 2024 - A Survey of Learning-based Automated Program Repair.pdf}
}

@misc{zhangSystematicLiteratureReview2024,
  title = {A {{Systematic Literature Review}} on {{Large Language Models}} for {{Automated Program Repair}}},
  author = {Zhang, Quanjun and Fang, Chunrong and Xie, Yang and Ma, YuXiang and Sun, Weisong and Yang, Yun and Chen, Zhenyu},
  year = {2024},
  month = may,
  number = {arXiv:2405.01466},
  eprint = {2405.01466},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.01466},
  urldate = {2025-06-26},
  abstract = {Automated Program Repair (APR) attempts to patch software bugs and reduce manual debugging efforts. Very recently, with the advances in Large Language Models (LLMs), an increasing number of APR techniques have been proposed, facilitating software development and maintenance and demonstrating remarkable performance. However, due to ongoing explorations in the LLM-based APR field, it is challenging for researchers to understand the current achievements, challenges, and potential opportunities. This work provides the first systematic literature review to summarize the applications of LLMs in APR between 2020 and 2024. We analyze 127 relevant papers from LLMs, APR and their integration perspectives. First, we categorize existing popular LLMs that are applied to support APR and outline three types of utilization strategies for their deployment. Besides, we detail some specific repair scenarios that benefit from LLMs, e.g., semantic bugs and security vulnerabilities. Furthermore, we discuss several critical aspects of integrating LLMs into APR research, e.g., input forms and open science. Finally, we highlight a set of challenges remaining to be investigated and the potential guidelines for future research. Overall, our paper provides a systematic overview of the research landscape to the APR community, helping researchers gain a comprehensive understanding of achievements and promote future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/S3NM3PEP/Zhang et al. - 2024 - A Systematic Literature Review on Large Language Models for Automated Program Repair.pdf;/Users/justingebert/Zotero/storage/ZRP8RU4F/2405.html}
}

@inproceedings{zhuSyntaxguidedEditDecoder2021,
  title = {A Syntax-Guided Edit Decoder for Neural Program Repair},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Zhu, Qihao and Sun, Zeyu and Xiao, Yuan-an and Zhang, Wenjie and Yuan, Kang and Xiong, Yingfei and Zhang, Lu},
  year = {2021},
  month = aug,
  pages = {341--353},
  publisher = {ACM},
  address = {Athens Greece},
  doi = {10.1145/3468264.3468544},
  urldate = {2025-07-04},
  isbn = {978-1-4503-8562-6},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/HAWUXGYW/Zhu et al. - 2021 - A syntax-guided edit decoder for neural program repair.pdf}
}

@phdthesis{zotero-item-336,
  type = {Phdthesis}
}
