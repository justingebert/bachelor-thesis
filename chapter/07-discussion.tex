In this section, we will discuss the results of the evaluation of our prototype and put them into context to answer the research questions. We begin by discussing the validity of our results, continue with the potentials and limitations of our approach and summarize the lessons learned. Finally, we outline a roadmap for future extensions of our work.

\section{Validity} \label{section:validity}

The evaluation results are very promising but face several limitations that affect their validity.

Since the repair process relies on \acp{LLM}, which are non-deterministic by design, executions may vary in their results. As the pipeline was only run twice per model, these results are not fully representative of each model's performance. Furthermore, the speed and availability of the tested \acp{LLM} heavily depends on the providers' \acp{API}, which can cause fluctuating execution times or even failures during periods of high traffic.

Repair costs are calculated based on token usage reported by the providers' \ac{API} responses. However, these numbers are not fully accurate, as providers can be non-transparent about actual token counts \cite{sunCoInCountingInvisible2025d}. As a result, the reported costs and execution times should be interpreted with caution and are not reliably comparable across providers.

Additionally, the system was executed on GitHub-hosted runners included in the free tier of GitHub. Therefore, the performance metrics reflect the behavior of GitHub's cloud-hosted CI environment. While this allows for rapid prototyping and testing, it limits the generalizability of absolute execution times and costs.

The evaluation is based on the QuixBugs benchmark, which includes 40 single-line bugs, each in a separate file. This dataset does not fully represent real-world software development scenarios, as it only covers a narrow set of small algorithmic bugs. Moreover, we assume that passing the provided test suite indicates a correct fix. While QuixBugs offers relatively thorough tests for its scale, they do not guarantee semantic correctness or full behavioral equivalence with the ground truth. Therefore, a generated fix may pass all tests but still not fully resolve the underlying bug by introducing subtle errors that are not detected by the tests.

Another important consideration is that QuixBugs is an open-source benchmark published prior to the release of the evaluated \acp{LLM}. It is possible that the dataset or some of its bugs may have appeared in the training data of certain models. While this cannot be verified, it represents a further threat to validity and may inflate model performance.

To partially offset these limitations, we evaluated twelve diverse \acp{LLM}. These models vary in capability, speed and cost, which provides insight into how different configurations might generalize to larger datasets or other development environments. However, to draw broader conclusions about real-world effectiveness, further evaluation on more complex benchmarks such as Defects4J or SWE-Bench is necessary.

The threats outlined above limit the scope of our conclusions. Additional testing other programming languages, codebases, and CI platforms is required to fully validate the approach at production scale. Nevertheless, the results demonstrate that an LLM-based automated bug fixing pipeline can be successfully integrated into a CI workflow and can achieve non-trivial repair rates with minimal time investment at relatively low cost.

\section{Potentials}

Section \ref{chapter:implementation} addresses RQ1 by demonstrating that LLM-based automated bug fixing can effectively be integrated into \ac{CI} workflows through the use of GitHub Actions and containerized APR logic. While the prototype was used in two repositories\footnote{Evaluated in the ``quixbugs-apr'' and tested in ``bugfix-ci'' (see Appendix \ref{chapter:appendix})}, the underlying approach is designed for portability and can be extended to any Python project with minimal effort. The use of a YAML configuration file enables rapid adaptation of the system to new repositories and environments. This modularity allows for ongoing adjustments and improvements to the bug-fixing system without requiring changes to the codebase, making the solution both flexible and maintainable.

By showcasing the resulting workflow in Section \ref{section:showcase}, we demonstrated that, once integrated, producing an automated bug fix requires only a single user action. For example, a developer can simply create and label an issue\footnote{The issue must be assigned the according label (e.g., \texttt{bug\_v01}) that the APR system is configured to process.}, or schedule a dispatch within the GitHub repository. This highlights the potential of the approach to fully automate the bug-fixing process, eliminating the need for developer intervention in the repair workflow. Issue creation for tracking features, adjustments, or bugs is fundamental components of agile development. \cite{abrahamssonAgileSoftwareDevelopment2017} By integrating with this workflow, our system allows automated bug fixes to be triggered directly from these standard agile practices.

The resulting workflow further indicates that this integration can actively support developers throughout the agile lifecycle by automating the design, development, and testing stages for bug fixes. Developers are left primarily with specifying requirements in the form of issues and reviewing the generated fixes, thus reducing their manual workload. Notably, the system is able to process and resolve issues even with minimal descriptions, as shown in our evaluation, which relied on issues with little detailed information. This allows developers to focus on more complex and creative tasks, while routine bug fixes are handled automatically. These findings are consistent with the work of Hou et al.~\cite{houLargeLanguageModels2024}, who states that \acp{LLM} can accelerate bug fixing and enhance both software reliability and maintainability.

For answering RQ2, the evaluation results in Section \ref{section:evaluation-results} demonstrate that the prototype can achieve a repair success rate of up to 100\% on the QuixBugs benchmark, with the best-performing model (o4-mini) successfully repairing all 40 bugs in the few-shot configuration. This highlights the effectiveness of the proposed approach for fixing single-file bugs within a CI environment. However, a deeper look into the collected data reveals that the repair success rate varies across different \acp{LLM} and configurations, indicating a strong dependence on the \ac{LLM} used for repair.

In the zero-shot configuration, where each issue receives only a single repair attempt, several models already achieve high repair success rates. For example, gemini-2.5-pro achieves a success rate of 95\% (38 out of 40 issues), while gpt-4.1-mini and claude-sonnet-4-0 reach 90\%. Lightweight models such as gemini-2.0-flash-lite and gemini-2.5-flash-lite achieve success rates of 82.5\% and 90\%, respectively. These results indicate that smaller models can still deliver strong performance in zero-shot settings, although the highest success rates are achieved by more capable models.

The main advantage of smaller models lies in their cost-effectiveness and rapid execution. For instance, gemini-2.5-flash-lite achieves a repair success rate of 90\% in the zero-shot setting, with an average cost per issue of just \$0.0002 and an average execution time of 5.07 seconds. Similarly, gemini-2.0-flash-lite repairs 82.5\% of issues at an extremely low cost of \$0.0001 per issue and an average time of 6.07 seconds. In comparison, larger models such as gemini-2.5-pro reach a 95\% repair rate but at higher costs (\$0.071 per issue) and longer execution times (70.09 seconds per issue). These findings indicate that smaller more cost-efficient models can be highly effective for automated bug fixing in CI environments, enabling frequent and rapid submissions of fixes at minimal cost.

Allowing a \ac{LLM} to make multiple repair attempts per issue (few-shot), using internal feedback loops, significantly improves repair success rates across all models. For example, with up to three attempts, the o4-mini model improves from 90\% to 100\% repair success, and gpt-4.1-mini increases from 95\% to 97.5\%. Smaller models such as gpt-4.1-nano show an even larger improvement, rising from 70\% in the single-attempt setting to 90\% with retries. Importantly, these increases in success rate are achieved with a rise in cost and execution time. As a result, even lightweight models can reach performance levels similar to larger models while maintaining the advantages of lower cost and faster execution. These findings of improved few-shot performance align with the observations of Brown et al.~\cite{brownLanguageModelsAre2020}, who demonstrated the effectiveness of few-shot learning for large language models.

The measured repair times across all models range from as low as 5 seconds per issue (e.g., gemini-2.5-flash-lite) up to about 70 seconds per issue (gemini-2.5-pro). When looking at the total execution time for the entire pipeline, the results show that \ac{CI} overhead is minimal. This indicates that the integration of LLM-based bug fixing into CI workflows does not introduce significant bottlenecks or computational overhead. All pipeline runs complete single issue repairs well within typical CI job expectations\footnote{Recent research suggests that 10 minutes is the most acceptable build duration \cite{hiltonTradeoffsContinuousIntegration2017}.}, meaning the approach does not impose significant waiting times for developers.

In terms of cost, repair attempts vary from as little as \$0.0001 per issue for lightweight models to around \$0.07 per issue for larger models. These results confirm that automated bug fixing with \acp{LLM} is both time-efficient and cost-effective, making it practical for integration into \ac{CI} environments without introducing workflow bottlenecks or major expenses.

Overall, the combination of high repair success rates, low execution times, and minimal costs demonstrates that this approach is both feasible and practical for automating bug fixing in CI environments. Because each repair task is isolated, multiple issues can be processed concurrently, further increasing throughput and minimizing delays for development.

By implementing a lightweight approach together with the interactivity of the GitHub platform, this system achieves repair success rates on the QuixBugs benchmark that match or exceed those reported in recent APR research~\cite{huCanGPTO1Kill2024,yinThinkRepairSelfDirectedAutomated2024, zhangSTEAMSimulatingInTeractive2025, xiaLessTrainingMore2022, leeUnifiedDebuggingApproach2024 }. In addition, the direct integration into the CI workflow provides transparent metrics for both cost and performance, offering practical advantages over many traditional APR approaches.

As highlighted in the results, system effectiveness and efficiency are highly dependent on the underlying \ac{LLM} used. However, the modular design of the approach allows for straightforward integration of improved or newly released models. This ensures that the system can benefit from advances in \ac{LLM} technology without requiring major changes to the overall pipeline. Its portability and modularity allow for a wide range of enhancements, such as integrating new \acp{LLM}, experimenting with different prompting strategies or incorporating richer feedback mechanisms, as described in in Section~\ref{section:roadmap}.

\section{Limitations}
Ultimately, there are also limitations faced by the implementation and the overall approach.

As mentioned in section \ref{section:validity}, the system is constrained to addressing small issues only. Even with small issues, the performance and availability of the system highly depend on external factors such as the \ac{LLM} providers' \acp{API} and the GitHub Actions runners. This presents a limitation in terms of reliability and performance in a real software development cycle.

Regarding integration with GitHub, the system faces additional limitations imposed by the GitHub Actions environment. Workflow runs cannot be skipped, and the filtering logic for events using external configuration data is limited. As a result, the workflow must execute on every issue labeling event to perform filtering in the first job. This causes the GitHub Actions tab to fill with runs that are marked as successful but did not process any bugs, resulting in a cluttered run history and potential confusion for users. This could be improved by migrating the APR Core to a GitHub App integration that listens to webhook events and only triggers the workflow for relevant issues.

Additionally, security and privacy concerns arise from the fact that the program repair relies on \acp{LLM}. Since the issue title and description are added to the prompt used for repair, malicious instructions could be inserted into an issue. Therefore, untrusted project contributors should not be allowed to create or edit issues. Furthermore, code submitted as a pull request must be carefully verified and reviewed before merging.

Lastly, \ac{LLM} providers like Google, OpenAI, and Anthropic are not fully transparent about their data and storage policies. This may raise concerns about the privacy of code and issues processed by the system, especially for private or sensitive repositories. While the prototype currently relies on commercial models, its modular design allows for straightforward integration of open source alternatives in the future. Nevertheless, copyright and licensing issues may arise when using code generated by \acp{LLM} trained on copyrighted data \cite{sauvolaFutureSoftwareDevelopment2024, houLargeLanguageModels2024}.

\section{Lessons Learned}
The development and evaluation of this prototype provided several valuable insights, both technical and practical:

\begin{itemize}
    \item \textbf{Effectiveness of \acp{LLM}:} Large Language Models can automate bug fixing in CI environments, but outcomes are highly dependent on the model's capabilities, context and prompting techniques.
    \item \textbf{Provider and API Reliability:} The \acp{API} of \ac{LLM} providers are not fully reliable, with some, such as Googles Gemini-2.5-pro, experiencing instabilities or high latency. Developing retry logic and proper error handling is essential for using external \acp{API}.
    \item \textbf{Rapid Evolution:} The landscape of AI is evolving quickly, with new models and approaches released on a daily basis. Following research and designing modular software are important to stay up to date in this field.
    \item \textbf{Benchmarking Challenges:} Benchmark datasets like QuixBugs provide a starting point for evaluation, but real-world applications will require broader benchmarks like SWE Bench with diverse scenarios to fully assess effectiveness.
    \item \textbf{Human Oversight Remains Important:} While automation can handle routine bugs, human review remains important, particularly for accessing fixes for edge cases, hallucinations and insecurities.
\end{itemize}

Overall, the implementation revealed both the potential and the current limitations of integrating LLM-based program repair into CI and provided valuable insights in the field of \ac{APR}.


\section{Roadmap for Extensions} \label{section:roadmap}
The modular and extensible design of this prototype allows for future improvements, both in research and practical application. The ideas presented in this section have emerged during implementation and while exploring the latest developments in \ac{APR} research discussed throughout this thesis. Due to time constraints, these directions were not realized, but they represent valuable opportunities for further work.

\begin{itemize}
    \item \textbf{Deeper Data Analysis:} The system already collects logs and metrics, which could be analyzed in greater detail to identify failures and improve localization and repair.
    \item \textbf{Optimization of Cost, Time, and Success Rates:} More extensive experimentation with models, context engineering and prompting strategies could help to further optimize costs and execution times while maximizing repair success rates.
    \item \textbf{Adaptive Model Selection:} Implementing a dynamic approach where lightweight models handle most repairs but challenging cases fall back to more powerful models could improve effectiveness and efficiency.
    \item \textbf{Broader and Richer Benchmarks:} Extending evaluation beyond QuixBugs to include larger and more diverse benchmarks, such as Defects4J or SWE-bench, would provide a stronger basis for generalization and reveal strengths and limitations in real-world settings.
    \item \textbf{Improved Platform Integration:} Integrating the system as a GitHub App that leverages webhook events, or using service accounts for better access control, could simplify development and improve user experience.
    \item \textbf{Experimentation with Agent Architectures:} Testing and comparing multi-agent or interactive agent approaches could uncover additional gains in repair effectiveness.
    \item \textbf{Human Factors and Developer Experience:} Future work could measure developer trust, satisfaction, and acceptance of automated fixes in real teams, supporting adoption in production workflows.
\end{itemize}

These directions illustrate just a few of the many ways the presented approach can serve as a foundation for continued innovation in \acf{APR}.
