In this section, we will discuss the results of the evaluation of our prototype and put them into context to answer the research questions. We begin by evaluating the validity of our findings, the potential of our approach, and its limitations, and summarize the lessons learned. Finally, we outline a roadmap for future extensions of our work.

\section{Validity} \label{section:validity}

The results from the evaluation are very promising but face several limitations that affect their validity.

Since the repair process relies on LLMs, which are non-deterministic by design, executions may vary in their results. As the pipeline was only run once per model, these results are not fully representative of each model's performance. Furthermore, the speed and availability of the tested LLMs heavily depend on the providers' APIs, which can cause fluctuating execution times or even failures during periods of high traffic.

Costs are calculated based on token usage reported by the providers' API responses. However, these figures are not fully accurate, as providers are typically non-transparent about actual token counts \cite{sunCoInCountingInvisible2025d}. As a result, the reported costs and execution times should be interpreted with caution and are not reliably comparable across providers.

Additionally, the system was executed on GitHub-hosted runners included in the free tier of GitHub. Therefore, the performance metrics reflect the behavior of GitHub's cloud-hosted CI environment. While this allows for rapid prototyping and testing, it limits the generalizability of absolute execution times and costs.

The evaluation is based on the QuixBugs benchmark, which includes 40 single-line bugs, each in a separate file. This dataset does not fully represent real-world software development scenarios, as it only covers a narrow set of small algorithmic bugs. Moreover, we assume that passing the provided test suite indicates a correct fix. While QuixBugs offers relatively thorough tests for its scale, they do not guarantee semantic correctness or full behavioral equivalence with the ground truth. Therefore, a generated fix may pass all tests while leaving the program semantically incorrect.

Another important consideration is that QuixBugs is an open-source benchmark published prior to the release of the evaluated LLMs. It is possible that the dataset or its individual bugs may have appeared in the training data of some models. While this cannot be verified, it represents a further threat to validity and may inflate model performance.

To partially offset these limitations, we evaluated twelve diverse LLMs. These models vary in capability, architecture, and cost, which provides insight into how different configurations might generalize to larger datasets or other CI environments. However, to draw broader conclusions about real-world effectiveness, further evaluation on more complex benchmarks such as Defects4J or SWE-Bench is necessary.

The threats outlined above limit the scope of our conclusions. Additional testing on other programming languages, codebases, and CI platforms is required to fully validate the approach at production scale. Nevertheless, the results demonstrate that an LLM-based automated bug fixing pipeline can be successfully integrated into a CI workflow and can achieve non-trivial repair rates with minimal time investment and relatively low cost.

\section{Potentials}

Section \ref{chapter:implementation} answers RQ1 by demonstrating how LLM based Automated Bug Fixing can be integrated into a CI workflow using GitHub Actions and containerized APR logic. A key advantage of this approach is, it allows for adjustments and further improvement without the need for major changes to the system.
The concept is applicable to other Python repositories but was only tested on the ``quixbugs-apr'' repo and the ``bugfix-ci'' development repository. The option for custom configuration makes it adaptable to different repositories and environments.

Section \ref{section:showcase} demonstrated that with this integration getting a automatically generated fix for a bug only requires a single user action. Creating and labeling an issue in the GitHub repository. This shows that the approach can take over and submit fixes, without the need for developer intervention. Creating issues/ticket for features, adjustments or bugs is a key part in agile frameworks for tracking tasks for in each iteration. %\cite{} 
The results indicate that this integration can support developers in an agile lifecycle by automating the design, development and test stages for a bug, leaving only requirement specification in form of issues and review of generated fixes to the developers. Furthermore issue descriptions for such bugs can be minimal, as the issues for evaluation contained no detailed information on the bug. This allows developers to focus on more complex tasks, while the system takes care of the simpler bugs. Our results align with the findings of \cite{houLargeLanguageModels2024}, who states that LLMs can accelerate bug fixing and enhance software reliability and maintainability.

The evaluation results in section \ref{section:evaluation-results} show that the prototype can achieve a repair success rate of up to 100\% on the QuixBugs benchmark with the best performing model ``o4-mini''. This indicates that the approach taken can be effective in fixing single file bugs in a CI environment. When taking a deeper look at the results, we observed that the repair success rate is highly dependent on the LLM model used.

With one attempt for every issue\footnote{zero shot prompting} the best performing models already achieve a repair success rate of 97.5\% on the QuixBugs benchmark, while smaller models like gemini-2.5-flash-lite gpt-4.1-mini achieve a repair success rate of 95\% and 90\% respectively. This shows that with zero shot prompting smaller models can achieve good results, but larger models are more effective in fixing bugs.

The potential of smaller models like: X,Y,Z lies in their performance and costs effectiveness. The results show that smaller models can achieve a repair success rate of 95\% with a significantly lower cost and execution time compared to larger models. For example X solves x\% with an average cost of X taking an average time of X per issue while Y repairs Y\% successful with only \$Y average per issue and 1/3 of the time per issue. This indicates that smaller models can be used to automate bug fixing in a CI environments, keeping costs low and performance resulting in quicker submissions of fixes.

The introduction of multi attempt fixes with an internal feedback loop, enhances the effectiveness of the integration significantly for every single model tested. By allowing the LLM to retry fix generation with additional context on failure, the repair success rates climb an average of X\% per model. Particularly smaller models benefit from the few shot approach because the success rates rise to factor of X while costs and execution times rise linearly. This shows that small models can archive similar performance to larger model with lower costs and better performance.

With average repair times reaching from x-Y per issue, the approach is feasible for use in a CI environment. Furthermore these times do not impose significant waiting times for developers and indicate that bugs can be fixed quickly and efficiently while developers focus on more important and complex tasks. With costs reaching from \$X to \$Y per issue, this approach is also relatively cost effective and affordable.

--zero shot inexpensive for bigger models while for smaller it makes sense for multi attempt

Overall repair success rates, execution times and costs demonstrate that the approach is feasible and can be used to automate bug fixing in a CI environment.
does not take significant time and furthermore it can be run concurrently

Leveraging paradigms of an agentless approach added with interactivity of the GitHub project platform we could achieve results that hold up with current APR research \cite{huCanGPTO1Kill2024, } showing similar repair success rates on the QuixBugs benchmark while providing an integrated approach with transparent cost and performance insights.

As mentioned before the performance is highly dependant on the underlying LLMs used. As companies like OpenAI, Google and Anthropic are constantly improving their models, the approach taken in this thesis can be extended to use future models as they become available. This means that the system can be improved over time without the need for major changes making the approach future proof and adaptable to new developments in the field of LLMs. The portability, modularity and extendability allows for future opportunities mentioned in \ref{section:roadmap}.

\section{Limitations}
Ultimately, there are also limitations faced by the prototype and the overall approach taken.

As mentioned in section \ref{section:validity}, the system is constrained to addressing small issues only. Even with small issues, the timing and availability of the system are highly dependent on external factors such as the LLM providers' APIs and the GitHub Actions runners. This presents a limitation in terms of reliability and performance in a real software development cycle.

Regarding integration with GitHub, the system faces additional limitations imposed by the GitHub Actions environment. Runs cannot be skipped, and the filtering logic for events using external configuration data is very limited. As a result, the workflow must execute on every issue labeling event to perform filtering in the first job. This causes the GitHub Actions tab to fill with runs that are marked as successful but do not process any bugs, resulting in a cluttered run history and potential confusion for users. This could be improved by migrating the APR core to a GitHub App that listens to webhook events and only triggers the workflow for relevant issues.

Additionally, security and privacy concerns arise from the fact that program repair relies on LLMs. Since the issue title and description are added to the prompt used for repair, malicious instructions could be inserted into an issue. Therefore, untrusted project contributors should not be allowed to create or edit issues. Furthermore, code submitted as a pull request to fix a bug must be carefully verified and reviewed before merging.

Large LLM providers like Google, OpenAI, and Anthropic are not fully transparent about their data and storage policies. This may raise concerns about the privacy of code and issues processed by the system, especially for private or sensitive repositories. The prototype was not tested using open source models, but it is designed to be modular and extendable for use with open source models. Nevertheless, copyright and licensing issues may arise when code is generated by LLMs trained on copyrighted data \cite{sauvolaFutureSoftwareDevelopment2024, houLargeLanguageModels2024}.

\section{Lessons Learned}
The development and evaluation of the prototype was an interesting and insightful experience. The following lessons were learned during the process:
- LLMs can be used to automate bug fixing in a CI environment, but the results are highly dependent on the quality of the LLMs.
- The apis of llm providers can be unreliable especially googles gemini- 2.5-pro.
- The field of LLMs is rapidly evolving, and new models are released frequently. This makes it difficult to keep up with the latest developments.


\section{Roadmap for Extensions} \label{section:roadmap}
As mentioned before the prototype developed in this thesis is modular and easily extendable. This allows for future opportunities and research. Below we list potentials for further extensions and analysis which was not implemented because of the contained time for thesis.

- dig further into collected data, see where it went wrong in the patches or localization, further analyze the collected data, we collected a lot of data which can be used for more insights
- test how cost, time and repair success can be optimized
- small model with fallback to larger model
- more complex prompt and context and prompt engineering
- richer benchmarks
- Service Accounts for transparent and better platform integration a
- github app which replies on webhook events for better integration
- test complex agent architectures and compare metrics and results
- measure developer trust and satisfaction
